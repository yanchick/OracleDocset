<!DOCTYPE html><html lang="en-US"><head>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1"/>
<meta charset="utf-8"/>
<meta http-equiv="Content-Type" content="UTF-8"/>
<a class="dashingAutolink" name="autolink-5570"></a><a class="dashAnchor" name="//apple_ref/cpp/Package/Decision%20Tree"></a><title>Decision Tree</title>
<meta name="generator" content="DITA Open Toolkit version 1.8.5 (Mode = doc)"/>
<meta name="description" content="Learn how to use Decision Tree algorithm. Decision Tree is one of the Classification algorithms that the Oracle Data Mining supports."/>
<meta name="keywords" content="transparency, model details, entropy, overfitting"/>
<meta name="dcterms.created" content="2017-05-15T23:08:00Z"/>
<meta name="robots" content="all"/>
<meta name="dcterms.title" content="Data Mining Concepts"/>
<meta name="dcterms.identifier" content="E17692-19"/>
<meta name="dcterms.isVersionOf" content="DMCON"/>
<meta name="dcterms.rights" content="Copyright&nbsp;&copy;&nbsp;2005, 2017, Oracle&nbsp;and/or&nbsp;its&nbsp;affiliates.&nbsp;All&nbsp;rights&nbsp;reserved."/>
<link rel="Start" href="../index.htm" title="Home" type="text/html"/>
<link rel="Copyright" href="../dcommon/html/cpyr.htm" title="Copyright" type="text/html"/>

<script type="application/javascript" src="../dcommon/js/headfoot.js"></script>
<script type="application/javascript" src="../nav/js/doccd.js" charset="UTF-8"></script>
<link rel="Contents" href="toc.htm" title="Contents" type="text/html"/>
<link rel="Index" href="index.htm" title="Index" type="text/html"/>
<link rel="Prev" href="GUID-B7D12599-FB4C-45E3-BCE4-E54A3C6F0E64.htm" title="Previous" type="text/html"/>
<link rel="Next" href="GUID-F4D117F3-FA0C-4CA4-9034-67D12339AE90.htm" title="Next" type="text/html"/>
<link rel="alternate" href="E17692-19.pdf" title="PDF version" type="application/pdf"/>
<link rel="schema.dcterms" href="http://purl.org/dc/terms/"/>
<link rel="stylesheet" href="../dcommon/css/fusiondoc.css"/>
<link rel="stylesheet" type="text/css" href="../dcommon/css/header.css"/>
<link rel="stylesheet" type="text/css" href="../dcommon/css/footer.css"/>
<link rel="stylesheet" type="text/css" href="../dcommon/css/fonts.css"/>
<link rel="stylesheet" href="../dcommon/css/foundation.css"/>
<link rel="stylesheet" href="../dcommon/css/codemirror.css"/>
<link rel="stylesheet" type="text/css" title="Default" href="../nav/css/html5.css"/>
<link rel="stylesheet" href="../dcommon/css/respond-480-tablet.css"/>
<link rel="stylesheet" href="../dcommon/css/respond-768-laptop.css"/>
<link rel="stylesheet" href="../dcommon/css/respond-1140-deskop.css"/>
<script type="application/javascript" src="../dcommon/js/modernizr.js"></script>
<script type="application/javascript" src="../dcommon/js/codemirror.js"></script>
<script type="application/javascript" src="../dcommon/js/jquery.js"></script>
<script type="application/javascript" src="../dcommon/js/foundation.min.js"></script>
<script type="text/javascript" src="/s7.addthis.com/js/300/addthis_widget.js#pubid=ra-552992c80ef99c8d" async="async"></script>
<script type="application/javascript" src="../dcommon/js/jqfns.js"></script>
<script type="application/javascript" src="../dcommon/js/ohc-inline-videos.js"></script>
<!-- Add fancyBox -->
<link rel="stylesheet" href="../dcommon/fancybox/jquery.fancybox.css?v=2.1.5" type="text/css" media="screen"/>
<script type="text/javascript" src="../dcommon/fancybox/jquery.fancybox.pack.js?v=2.1.5"></script>
<!-- Optionally add helpers - button, thumbnail and/or media -->
<link rel="stylesheet" href="../dcommon/fancybox/helpers/jquery.fancybox-buttons.css?v=1.0.5" type="text/css" media="screen"/>
<script type="text/javascript" src="../dcommon/fancybox/helpers/jquery.fancybox-buttons.js?v=1.0.5"></script>
<script type="text/javascript" src="../dcommon/fancybox/helpers/jquery.fancybox-media.js?v=1.0.6"></script>
<link rel="stylesheet" href="../dcommon/fancybox/helpers/jquery.fancybox-thumbs.css?v=1.0.7" type="text/css" media="screen"/>
<script type="text/javascript" src="../dcommon/fancybox/helpers/jquery.fancybox-thumbs.js?v=1.0.7"></script>
</head>
<body>
<a href="#BEGIN" class="accessibility-top skipto" tabindex="0">Go to main content</a><header><!--
<div class="zz-skip-header"><a id="top" href="#BEGIN">Go to main content</a>--></header>
<div class="row" id="CONTENT">
<div class="IND large-9 medium-8 columns" dir="ltr">
<a id="BEGIN" name="BEGIN"></a>
<a id="GUID-14DE1A88-220F-44F0-9AC8-77CA844D4A63"></a> <span id="PAGE" style="display:none;">18/29</span> <!-- End Header -->
<a id="DMCON019"></a>
<h1 id="DMCON-GUID-14DE1A88-220F-44F0-9AC8-77CA844D4A63" class="sect1"><span class="enumeration_chapter">11</span> Decision Tree</h1>
<div>
<p>Learn how to use Decision Tree algorithm. Decision Tree is one of the Classification algorithms that the Oracle Data Mining supports.</p>
<p><a id="d14309e19" class="indexterm-anchor"></a><a id="d14309e21" class="indexterm-anchor"></a><a id="d14309e25" class="indexterm-anchor"></a></p>
<ul style="list-style-type: disc;">
<li>
<p><a href="GUID-14DE1A88-220F-44F0-9AC8-77CA844D4A63.htm#GUID-7ABEA3A8-126C-42A3-BD3B-95DFF2F69A16">About Decision Tree</a></p>
</li>
<li>
<p><a href="GUID-14DE1A88-220F-44F0-9AC8-77CA844D4A63.htm#GUID-542B8B8C-0327-46E3-873A-7C71C6855531">Growing a Decision Tree</a></p>
</li>
<li>
<p><a href="GUID-14DE1A88-220F-44F0-9AC8-77CA844D4A63.htm#GUID-A04498CC-C4C2-415D-80B5-B02629EAF1E9">Tuning the Decision Tree Algorithm</a></p>
</li>
<li>
<p><a href="GUID-14DE1A88-220F-44F0-9AC8-77CA844D4A63.htm#GUID-D971596B-1AD0-4639-8398-05090A49835E">Data Preparation for Decision Tree</a></p>
</li>
</ul>
<div class="infoboxnotealso" id="GUID-14DE1A88-220F-44F0-9AC8-77CA844D4A63__GUID-9A58073E-C84C-4613-BC67-1CDAD0EAE7D9">
<p class="notep1">See Also:</p>
<p><span class="q">&#34;<a href="GUID-3D51EC47-E686-4468-8F49-A27B5F8E8FE4.htm#GUID-3D51EC47-E686-4468-8F49-A27B5F8E8FE4" title="Learn how to predict a categorical target through Classification - the supervised mining function.">Classification</a>&#34;</span></p>
</div>
</div>
<a id="DMCON296"></a>
<div class="props_rev_3"><a id="GUID-7ABEA3A8-126C-42A3-BD3B-95DFF2F69A16"></a>
<h2 id="DMCON-GUID-7ABEA3A8-126C-42A3-BD3B-95DFF2F69A16" class="sect2">About Decision Tree</h2>
<div>
<p>The Decision Tree algorithm, like Naive Bayes, is based on conditional probabilities. Unlike Naive Bayes, decision trees generate <strong class="term">rules</strong>. A rule is a conditional statement that can be understood by humans and used within a database to identify a set of records.</p>
<p>In some applications of data mining, the reason for predicting one outcome or another may not be important in evaluating the overall quality of a model. In others, the ability to explain the reason for a decision can be crucial. For example, a Marketing professional requires complete descriptions of customer segments to launch a successful marketing campaign. The Decision Tree algorithm is ideal for this type of application.</p>
<p>Use Decision Tree rules to validate models. If the rules make sense to a subject matter expert, then this validates the model.</p>
</div>
<a id="DMCON298"></a><a id="DMCON297"></a>
<div class="props_rev_3"><a id="GUID-D0C6B5AC-A75D-4AA4-8809-F416D66D33C7"></a>
<h3 id="DMCON-GUID-D0C6B5AC-A75D-4AA4-8809-F416D66D33C7" class="sect3">Decision Tree Rules</h3>
<div>
<p>Oracle Data Mining supports several algorithms that provide <a id="d14309e108" class="indexterm-anchor"></a>rules. In addition to decision trees, clustering algorithms provide rules that describe the conditions shared by the members of a cluster, and association rules provide rules that describe associations between attributes.</p>
<p>Rules provide <strong class="term">model transparency</strong>, a window on the inner workings of the model. Rules show the basis for the model&#39;s predictions. Oracle Data Mining supports a high level of model transparency. While some algorithms provide rules, <span class="italic">all</span> algorithms provide <strong class="term">model details</strong>. You can examine model details to determine how the algorithm handles the attributes internally, including transformations and reverse transformations. Transparency is discussed in the context of data preparation in <a class="olink DMPRG580" target="_blank" href="../DMPRG/GUID-CDBD54E4-B47C-4C33-8FEE-47FDFE96EAC7.htm#DMPRG580">&#34;Understanding Reverse Transformations&#34;</a> and in the context of model building in <a class="olink DMPRG216" target="_blank" href="../DMPRG/GUID-DE4E77DD-6974-454D-9B21-BD830F2EAD5F.htm#DMPRG216">&#34;Viewing Model Details&#34;</a> in <span class="italic">Oracle Data Mining User&#39;s Guide</span>.</p>
<p>The following figure shows a rule generated by a Decision Tree model. This rule comes from a decision tree that predicts the probability that customers increase spending if given a loyalty card. A target value of 0 means not likely to increase spending; 1 means likely to increase spending.</p>
<div class="figure" id="GUID-D0C6B5AC-A75D-4AA4-8809-F416D66D33C7__BGBDGCFI">
<p class="titleinfigure">Figure 11-1 Sample Decision Tree Rule</p>
<img width="470" height="184" src="img/GUID-031BF16B-D547-4B58-B30B-E95738ACF78A-default.gif" alt="Description of Figure 11-1 follows" title="Description of Figure 11-1 follows"/><br/>
<a href="img_text/GUID-031BF16B-D547-4B58-B30B-E95738ACF78A-default.htm">Description of &#34;Figure 11-1 Sample Decision Tree Rule&#34;</a></div>
<!-- class="figure" -->
<p>The rule shown in <a href="GUID-14DE1A88-220F-44F0-9AC8-77CA844D4A63.htm#GUID-D0C6B5AC-A75D-4AA4-8809-F416D66D33C7__BGBDGCFI">Figure 11-1</a> represents the conditional statement:</p>
<pre dir="ltr">IF 
          (current residence &gt; 3.5 and has college degree and is single) 
THEN
          predicted target value = 0
</pre>
<p>This rule is a full rule. A surrogate rule is a related attribute that can be used at apply time if the attribute needed for the split is missing.</p>
<div class="infoboxnotealso" id="GUID-D0C6B5AC-A75D-4AA4-8809-F416D66D33C7__GUID-AAA3FB42-C5CF-423B-A01D-72A0654D9AE8">
<p class="notep1">See Also:</p>
<ul style="list-style-type: disc;">
<li>
<p><span class="q">&#34;<a href="GUID-7FB17270-54F6-4898-A4F9-319CD94B450B.htm#GUID-7FB17270-54F6-4898-A4F9-319CD94B450B" title="Learn how to discover natural groupings in the data through Clustering - the unsupervised mining function.">Clustering</a>&#34;</span></p>
</li>
<li>
<p><span class="q">&#34;<a href="GUID-491998B3-B92B-4F84-8A79-94780B8AFD0C.htm#GUID-491998B3-B92B-4F84-8A79-94780B8AFD0C" title="Learn how to discover Association Rules through Association - an unsupervised mining function.">Association</a>&#34;</span></p>
</li>
</ul>
</div>
</div>
<a id="DMCON299"></a>
<div class="props_rev_3"><a id="GUID-5AE37A6A-BC38-411E-92B4-A9065A568B2A"></a>
<h4 id="DMCON-GUID-5AE37A6A-BC38-411E-92B4-A9065A568B2A" class="sect4">Confidence and Support</h4>
<div>
<p>Confidence and support are properties of rules. These statistical measures can be used to rank the rules and hence the predictions.</p>
<p><strong class="term">Support</strong>: The number of records in the training data set that satisfy the rule.</p>
<p><strong class="term">Confidence</strong>: The likelihood of the predicted outcome, given that the rule has been satisfied.</p>
<p>For example, consider a list of 1000 customers (1000 cases). Out of all the customers, 100 satisfy a given rule. Of these 100, 75 are likely to increase spending, and 25 are not likely to increase spending. The <strong class="term">support of the rule</strong> is 100/1000 (10%). The <strong class="term">confidence of the prediction</strong> (likely to increase spending) for the cases that satisfy the rule is 75/100 (75%).</p>
</div>
</div>
</div>
<a id="DMCON300"></a>
<div class="props_rev_3"><a id="GUID-2A83D176-3B04-4C04-90C1-96F64AB4B680"></a>
<h3 id="DMCON-GUID-2A83D176-3B04-4C04-90C1-96F64AB4B680" class="sect3">Advantages of Decision Trees</h3>
<div>
<p>The Decision Tree algorithm produces accurate and interpretable models with relatively little user intervention. The algorithm can be used for both binary and multiclass classification problems.</p>
<p>The algorithm is fast, both at build time and apply time. The build process for Decision Tree supports <a id="d14309e227" class="indexterm-anchor"></a>parallel execution. (Scoring supports parallel execution irrespective of the algorithm.) For information about parallel execution, refer to <a class="olink VLDBG010" target="_blank" href="../VLDBG/GUID-3E2AE088-2505-465E-A8B2-AC38813EA355.htm#VLDBG010"><span class="italic">Oracle Database VLDB and Partitioning Guide</span></a>.</p>
<p>Decision Tree scoring is especially fast. The tree structure, created in the model build, is used for a series of simple tests, (typically 2-7). Each test is based on a single predictor. It is a membership test: either IN or NOT IN a list of values (categorical predictor); or LESS THAN or EQUAL TO some value (numeric predictor).</p>
</div>
</div>
<a id="DMCON306"></a>
<div class="props_rev_3"><a id="GUID-B25FBEC2-1B4D-49EE-95E1-2E7D6A4F8E28"></a>
<h3 id="DMCON-GUID-B25FBEC2-1B4D-49EE-95E1-2E7D6A4F8E28" class="sect3">XML for Decision Tree Models</h3>
<div>
<p>You can generate XML representing a decision tree model; the generated XML satisfies the definition specified in the Data Mining Group Predictive Model Markup Language (PMML) version 2.1 specification. The XML specification is available at <a href="http://www.dmg.org" target="_blank"><code class="codeph">http://www.dmg.org</code></a>.</p>
</div>
</div>
</div>
<a id="DMCON302"></a><a id="DMCON301"></a>
<div class="props_rev_3"><a id="GUID-542B8B8C-0327-46E3-873A-7C71C6855531"></a>
<h2 id="DMCON-GUID-542B8B8C-0327-46E3-873A-7C71C6855531" class="sect2">Growing a Decision Tree</h2>
<div>
<p>A Decision Tree predicts a target value by asking a sequence of questions. At a given stage in the sequence, the question that is asked depends upon the answers to the previous questions. The goal is to ask questions that, taken together, uniquely identify specific target values. Graphically, this process forms a tree structure.</p>
<div class="figure" id="GUID-542B8B8C-0327-46E3-873A-7C71C6855531__BGBBGCGF">
<p class="titleinfigure">Figure 11-2 Sample Decision Tree</p>
<img width="577" height="313" src="img/GUID-01241931-D160-41D9-8364-10C6607C244B-default.gif" alt="Description of Figure 11-2 follows" title="Description of Figure 11-2 follows"/><br/>
<a href="img_text/GUID-01241931-D160-41D9-8364-10C6607C244B-print.htm">Description of &#34;Figure 11-2 Sample Decision Tree&#34;</a></div>
<!-- class="figure" -->
<p><a href="GUID-14DE1A88-220F-44F0-9AC8-77CA844D4A63.htm#GUID-542B8B8C-0327-46E3-873A-7C71C6855531__BGBBGCGF">Figure 11-2</a> is a Decision Tree with nine nodes (and nine corresponding rules). The target attribute is binary: 1 if the customer increases spending, 0 if the customer does not increase spending. The first split in the tree is based on the <code class="codeph">CUST_MARITAL_STATUS</code> attribute. The root of the tree (node 0) is split into nodes 1 and 3. Married customers are in node 1; single customers are in node 3.</p>
<p>The rule associated with node 1 is:</p>
<pre dir="ltr">Node 1 recordCount=712,0 Count=382, 1 Count=330
CUST_MARITAL_STATUS isIN  &#34;Married&#34;,surrogate:HOUSEHOLD_SIZE isIn &#34;3&#34;&#34;4-5&#34;
</pre>
<p>Node 1 has 712 records (cases). In all 712 cases, the <code class="codeph">CUST_MARITAL_STATUS</code> attribute indicates that the customer is married. Of these, 382 have a target of 0 (not likely to increase spending), and 330 have a target of 1 (likely to increase spending).</p>
</div>
<a id="DMCON303"></a>
<div class="props_rev_3"><a id="GUID-17616E59-425C-4D69-9E60-8C2F38E44CD0"></a>
<h3 id="DMCON-GUID-17616E59-425C-4D69-9E60-8C2F38E44CD0" class="sect3">Splitting</h3>
<div>
<p>During the training process, the Decision Tree algorithm must repeatedly find the most efficient way to split a set of cases (records) into two child nodes. Oracle Data Mining offers two homogeneity metrics, <strong class="term">gini</strong> and <strong class="term">entropy</strong>, for calculating the splits. The default metric is gini.</p>
<p>Homogeneity metrics asses the quality of alternative split conditions and select the one that results in the most homogeneous child nodes. Homogeneity is also called <strong class="term">purity</strong>; it refers to the degree to which the resulting child nodes are made up of cases with the same target value. The objective is to maximize the purity in the child nodes. For example, if the target can be either yes or no (does or does not increase spending), the objective is to produce nodes where most of the cases either increase spending or most of the cases do not increase spending.</p>
</div>
</div>
<a id="DMCON304"></a>
<div class="props_rev_3"><a id="GUID-D851A92F-67C2-4CA3-A427-615881A38AE3"></a>
<h3 id="DMCON-GUID-D851A92F-67C2-4CA3-A427-615881A38AE3" class="sect3">Cost Matrix</h3>
<div>
<p>All classification algorithms, including Decision Tree, support a <a id="d14309e360" class="indexterm-anchor"></a>cost-benefit matrix at apply time. You can use the same cost matrix for building and scoring a Decision Tree model, or you can specify a different cost/benefit matrix for scoring.</p>
<div class="infoboxnotealso" id="GUID-D851A92F-67C2-4CA3-A427-615881A38AE3__GUID-E264854D-F929-4361-8386-CFF292D32AAA">
<p class="notep1">See Also:</p>
<ul style="list-style-type: disc;">
<li>
<p><span class="q">&#34;<a href="GUID-3D51EC47-E686-4468-8F49-A27B5F8E8FE4.htm#GUID-F28D1480-ED10-4634-A566-F4D3CD17BDD7">Costs</a>&#34;</span></p>
</li>
<li>
<p><span class="q">&#34;<a href="GUID-3D51EC47-E686-4468-8F49-A27B5F8E8FE4.htm#GUID-590DD2C5-1BA5-40A3-9E3E-92AA2AE1D0EC">Priors and Class Weights</a>&#34;</span></p>
</li>
</ul>
</div>
</div>
</div>
<a id="DMCON305"></a>
<div class="props_rev_3"><a id="GUID-D42FB994-F4A6-4CD3-899C-0696DB4D25BD"></a>
<h3 id="DMCON-GUID-D42FB994-F4A6-4CD3-899C-0696DB4D25BD" class="sect3">Preventing Over-Fitting</h3>
<div>
<p>In principle, Decision Tree algorithms can grow each branch of the tree just deeply enough to perfectly classify the training examples. While this is sometimes a reasonable strategy, in fact it can lead to difficulties when there is noise in the data, or when the number of training examples is too small to produce a representative sample of the true target function. In either of these cases, this simple algorithm can produce trees that over-fit the training examples. Over-fit is a condition where a model is able to accurately predict the data used to create the model, but does poorly on new data presented to it.</p>
<p>To prevent over-fitting, Oracle Data Mining supports automatic <strong class="term">pruning</strong> and configurable <strong class="term">limit conditions</strong> that control tree growth. Limit conditions prevent further splits once the conditions have been satisfied. Pruning removes branches that have insignificant predictive power.</p>
</div>
</div>
</div>
<a id="DMCON307"></a>
<div class="props_rev_3"><a id="GUID-A04498CC-C4C2-415D-80B5-B02629EAF1E9"></a>
<h2 id="DMCON-GUID-A04498CC-C4C2-415D-80B5-B02629EAF1E9" class="sect2">Tuning the Decision Tree Algorithm</h2>
<div>
<p>The Decision Tree algorithm is implemented with reasonable defaults for splitting and termination criteria. However several build settings are available for fine tuning.</p>
<p>You can specify a homogeneity metric for finding the optimal split condition for a tree. The default metric is gini. The entropy metric is also available.</p>
<p>Settings for controlling the growth of the tree are also available. You can specify the maximum depth of the tree, the minimum number of cases required in a child node, the minimum number of cases required in a node in order for a further split to be possible, the minimum number of cases in a child node, and the minimum number of cases required in a node in order for a further split to be possible.</p>
<p>The training data attributes are binned as part of the algorithm&#39;s data preparation. You can alter the number of bins used by the binning step. There is a trade-off between the number of bins used and the time required for the build.</p>
<div class="infoboxnotealso" id="GUID-A04498CC-C4C2-415D-80B5-B02629EAF1E9__GUID-42AB5D0C-39E7-405F-ABF0-46A70D517E1F">
<p class="notep1">See Also:</p>
<p><a class="olink ARPLS610" target="_blank" href="../ARPLS/d_datmin.htm#ARPLS610"><span><cite>Oracle Database PL/SQL Packages and Types Reference</cite></span></a></p>
</div>
</div>
</div>
<a id="DMCON308"></a>
<div class="props_rev_3"><a id="GUID-D971596B-1AD0-4639-8398-05090A49835E"></a>
<h2 id="DMCON-GUID-D971596B-1AD0-4639-8398-05090A49835E" class="sect2">Data Preparation for Decision Tree</h2>
<div>
<p>The Decision Tree algorithm manages its own data preparation internally. It does not require pretreatment of the data. Decision Tree is not affected by Automatic Data Preparation.</p>
<div class="infoboxnotealso" id="GUID-D971596B-1AD0-4639-8398-05090A49835E__GUID-58C7DEB8-7A08-4ED5-B023-86A69B398B72">
<p class="notep1">See Also:</p>
<p><span class="q">&#34;Preparing the Data&#34;</span> in <a class="olink DMPRG005" target="_blank" href="../DMPRG/GUID-E1AB599C-1921-4BD7-B06B-FC466180A460.htm#DMPRG005"><span><cite>Oracle Data Mining User&rsquo;s Guide</cite></span></a></p>
<p><span class="q">&#34;Transforming the Data&#34;</span> in <a class="olink DMPRG578" target="_blank" href="../DMPRG/GUID-C3FDDEC7-8CC9-4AC1-A6C3-75D91E26B703.htm#DMPRG578"><span><cite>Oracle Data Mining User&rsquo;s Guide</cite></span></a></p>
</div>
</div>
</div>
</div>
<!-- class="ind" --><!-- Start Footer -->
</div>
<!-- add extra wrapper close div-->
<footer><!--
<hr />
<table class="cellalignment4116">
<tr>
<td class="cellalignment4123">
<table class="cellalignment4121">
<tr>
<td class="cellalignment4120"><a href="GUID-B7D12599-FB4C-45E3-BCE4-E54A3C6F0E64.htm"><img width="24" height="24" src="../dcommon/gifs/leftnav.gif" alt="Go to previous page" /><br />
<span class="icon">Previous</span></a></td>
<td class="cellalignment4120"><a href="GUID-F4D117F3-FA0C-4CA4-9034-67D12339AE90.htm"><img width="24" height="24" src="../dcommon/gifs/rightnav.gif" alt="Go to next page" /><br />
<span class="icon">Next</span></a></td>
</tr>
</table>
</td>
<td class="cellalignment-copyrightlogo"><img width="144" height="18" src="../dcommon/gifs/oracle.gif" alt="Oracle" /><br />
Copyright&nbsp;&copy;&nbsp;2005, 2017, Oracle&nbsp;and/or&nbsp;its&nbsp;affiliates.&nbsp;All&nbsp;rights&nbsp;reserved.<br />
<a href="../dcommon/html/cpyr.htm">Legal Notices</a></td>
<td class="cellalignment4125">
<table class="cellalignment4119">
<tr>
<td class="cellalignment4120"><a href="../index.htm"><img width="24" height="24" src="../dcommon/gifs/doclib.gif" alt="Go to Documentation Home" /><br />
<span class="icon">Home</span></a></td>
<td class="cellalignment4120"><a href="../nav/portal_booklist.htm"><img width="24" height="24" src="../dcommon/gifs/booklist.gif" alt="Go to Book List" /><br />
<span class="icon">Book List</span></a></td>
<td class="cellalignment4120"><a href="toc.htm"><img width="24" height="24" src="../dcommon/gifs/toc.gif" alt="Go to Table of Contents" /><br />
<span class="icon">Contents</span></a></td>
<td class="cellalignment4120"><a href="index.htm"><img width="24" height="24" src="../dcommon/gifs/index.gif" alt="Go to Index" /><br />
<span class="icon">Index</span></a></td>
<td class="cellalignment4120"><a href="../nav/mindx.htm"><img width="24" height="24" src="../dcommon/gifs/masterix.gif" alt="Go to Master Index" /><br />
<span class="icon">Master Index</span></a></td>
<td class="cellalignment4120"><a href="../dcommon/html/feedback.htm"><img width="24" height="24" src="../dcommon/gifs/feedbck2.gif" alt="Go to Feedback page" /><br />
<span class="icon">Contact Us</span></a></td>
</tr>
</table>
</td>
</tr>
</table>
--></footer>
<noscript>
<p>Scripting on this page enhances content navigation, but does not change the content in any way.</p>
</noscript>


</body></html>