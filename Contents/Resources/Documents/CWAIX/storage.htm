<!DOCTYPE html><html lang="en"><head>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1"/>
<meta charset="utf-8"/>
<a class="dashingAutolink" name="autolink-4876"></a><a class="dashAnchor" name="//apple_ref/cpp/Package/Configuring%20Storage%20for%20Oracle%20Grid%20Infrastructure%20and%20Oracle%20RAC"></a><title>Configuring Storage for Oracle Grid Infrastructure and Oracle RAC</title>
<meta name="generator" content="Oracle DARB XHTML Converter (Mode = document) - Merged Version 1093"/>
<meta name="dcterms.created" content="2017-07-23T23:35:19Z"/>
<meta name="robots" content="all"/>
<meta name="dcterms.title" content="Grid Infrastructure Installation Guide"/>
<meta name="dcterms.identifier" content="E51133-07"/>
<meta name="dcterms.isVersionOf" content="CWAIX"/>
<meta name="dcterms.rights" content="Copyright&nbsp;&copy;&nbsp;2014, 2017,&nbsp;Oracle&nbsp;and/or&nbsp;its&nbsp;affiliates.&nbsp;All&nbsp;rights&nbsp;reserved."/>
<link rel="Start" href="../index.htm" title="Home" type="text/html"/>
<link rel="Copyright" href="../dcommon/html/cpyr.htm" title="Copyright" type="text/html"/>

<script type="application/javascript" src="../dcommon/js/headfoot.js"></script>
<script type="application/javascript" src="../nav/js/doccd.js"></script>
<link rel="Contents" href="toc.htm" title="Contents" type="text/html"/>
<link rel="Index" href="index.htm" title="Index" type="text/html"/>
<link rel="Prev" href="usrgrps.htm" title="Previous" type="text/html"/>
<link rel="Next" href="crsunix.htm" title="Next" type="text/html"/>
<link rel="alternate" href="E51133-07.pdf" title="PDF version" type="application/pdf"/>
<link rel="schema.dcterms" href="http://purl.org/dc/terms/"/>
<link rel="stylesheet" href="../dcommon/css/fusiondoc.css"/>
<link rel="stylesheet" type="text/css" href="../dcommon/css/header.css"/>
<link rel="stylesheet" type="text/css" href="../dcommon/css/footer.css"/>
<link rel="stylesheet" type="text/css" href="../dcommon/css/fonts.css"/>
<link rel="stylesheet" href="../dcommon/css/foundation.css"/>
<link rel="stylesheet" href="../dcommon/css/codemirror.css"/>
<link rel="stylesheet" type="text/css" title="Default" href="../nav/css/html5.css"/>
<link rel="stylesheet" href="../dcommon/css/respond-480-tablet.css"/>
<link rel="stylesheet" href="../dcommon/css/respond-768-laptop.css"/>
<link rel="stylesheet" href="../dcommon/css/respond-1140-deskop.css"/>
<script type="application/javascript" src="../dcommon/js/modernizr.js"></script>
<script type="application/javascript" src="../dcommon/js/codemirror.js"></script>
<script type="application/javascript" src="../dcommon/js/jquery.js"></script>
<script type="application/javascript" src="../dcommon/js/foundation.min.js"></script>
<script type="text/javascript" src="/s7.addthis.com/js/300/addthis_widget.js#pubid=ra-552992c80ef99c8d" async="async"></script>
<script type="application/javascript" src="../dcommon/js/jqfns.js"></script>
<script type="application/javascript" src="../dcommon/js/ohc-inline-videos.js"></script>
<!-- Add fancyBox -->
<link rel="stylesheet" href="../dcommon/fancybox/jquery.fancybox.css?v=2.1.5" type="text/css" media="screen"/>
<script type="text/javascript" src="../dcommon/fancybox/jquery.fancybox.pack.js?v=2.1.5"></script>
<!-- Optionally add helpers - button, thumbnail and/or media -->
<link rel="stylesheet" href="../dcommon/fancybox/helpers/jquery.fancybox-buttons.css?v=1.0.5" type="text/css" media="screen"/>
<script type="text/javascript" src="../dcommon/fancybox/helpers/jquery.fancybox-buttons.js?v=1.0.5"></script>
<script type="text/javascript" src="../dcommon/fancybox/helpers/jquery.fancybox-media.js?v=1.0.6"></script>
<link rel="stylesheet" href="../dcommon/fancybox/helpers/jquery.fancybox-thumbs.css?v=1.0.7" type="text/css" media="screen"/>
<script type="text/javascript" src="../dcommon/fancybox/helpers/jquery.fancybox-thumbs.js?v=1.0.7"></script>
</head>
<body>
<a href="#BEGIN" class="accessibility-top skipto" tabindex="0">Go to main content</a><header><!--
<div class="zz-skip-header"><a id="top" href="#BEGIN">Go to main content</a>--></header>
<div class="row" id="CONTENT">
<div class="IND large-9 medium-8 columns" dir="ltr">
<a id="BEGIN" name="BEGIN"></a>
<span id="PAGE" style="display:none;">11/20</span> <!-- End Header -->
<script>
<!-- // <![CDATA[
window.name='storage'
// ]]> -->
</script> <script>
// <![CDATA[
function footdisplay(footnum,footnote) {
    var msg = window.open('', 'NewWindow' + footnum,
        'directories=no,height=100,location=no,menubar=no,resizable=yes,' +
        'scrollbars=yes,status=no,toolbar=no,width=598');
    msg.document.open('text/html');
    msg.document.write('<!DOCTYPE html ');
    msg.document.write('PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" ');

    msg.document.write('"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">');
    msg.document.write('<html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>');
    msg.document.write('Footnote&nbsp; ' + footnum);
    msg.document.write('<\/title><meta http-equiv="Content-Type" ');
    msg.document.write('content="text/html; charset=utf-8" />');
    msg.document.write('');
    msg.document.write('<style> <![CDATA[ ');
    msg.document.write('h1 {text-align: center; font-size: 14pt;}');
    msg.document.write('fieldset {border: none;}');
    msg.document.write('form {text-align: center;}');
    msg.document.write(' ]]\u003e <\/style>');
    msg.document.write('<\/head><body><div id="footnote"><h1>Footnote&nbsp; ' + footnum + '<\/h1><p>');
    msg.document.write(footnote);
    msg.document.write('<\/p><form action="" method="post"><fieldset>');
    msg.document.write('<input type="button" value="OK" ');
    msg.document.write('onclick="window.close();" />');
    msg.document.write('<\/fieldset><\/form><\/div><\/body><\/html>');
    msg.document.close();
    setTimeout(function() { var height = msg.document.getElementById('footnote').offsetHeight; msg.resizeTo(598, height + 100); }, 100);
    msg.focus();
}
// ]]>
</script> <noscript>
<p>The script content on this page is for navigation purposes only and does not alter the content in any way.</p>
</noscript>
<div id="CWAIX003" class="chapter"><a id="CHDFFHEC"></a>
<h1 class="chapter"><span class="secnum">6</span> Configuring Storage for Oracle Grid Infrastructure and Oracle RAC</h1>
<p>This chapter describes the storage configuration tasks that you must complete before you start the installer to install Oracle Clusterware and Oracle Automatic Storage Management (Oracle ASM), and that you must complete before adding an Oracle Real Application Clusters (Oracle RAC) installation to the cluster.</p>
<p>This chapter contains the following topics:</p>
<ul>
<li>
<p><a href="#BABHIJDF">Reviewing Oracle Grid Infrastructure Storage Options</a></p>
</li>
<li>
<p><a href="#BABHCHIB">About Shared File System Storage Configuration</a></p>
</li>
<li>
<p><a href="#BABCCCID">Configuring Operating System and Direct NFS Client</a></p>
</li>
<li>
<p><a href="#CDEDIGAH">Oracle Automatic Storage Management Storage Configuration</a></p>
</li>
</ul>
<a id="BABHIJDF"></a>
<div id="CWAIX238" class="sect1"><!-- infolevel="all" infotype="General" -->
<h2 class="sect1"><span class="secnum">6.1</span> Reviewing Oracle Grid Infrastructure Storage Options</h2>
<p>This section describes supported options for storing Oracle Grid Infrastructure for a cluster storage options. It contains the following sections:</p>
<ul>
<li>
<p><a href="#CDEFBCDD">Supported Storage Options</a></p>
</li>
<li>
<p><a href="#BABCAJAI">Oracle ACFS and Oracle ADVM</a></p>
</li>
<li>
<p><a href="#CDEBIEHA">General Storage Considerations for Oracle Grid Infrastructure and Oracle RAC</a></p>
</li>
<li>
<p><a href="#CHDJBEDC">Guidelines for Using Oracle ASM Disk Groups for Storage</a></p>
</li>
<li>
<p><a href="#CHDDEFIB">Using Logical Volume Managers with Oracle Grid Infrastructure and Oracle RAC</a></p>
</li>
<li>
<p><a href="#CDEEFHJC">After You Have Selected Disk Storage Options</a></p>
</li>
</ul>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
The Oracle Certification site on My Oracle Support for the most current information about certified storage options:
<pre><a href="https://support.oracle.com">https://support.oracle.com</a>
</pre></div>
<a id="CDEFBCDD"></a>
<div id="CWAIX247" class="sect2">
<h3 class="sect2"><span class="secnum">6.1.1</span> Supported Storage Options</h3>
<p>The following table shows the storage options supported for storing Oracle Clusterware and Oracle RAC files.</p>
<div id="CWAIX248" class="tblformalwidemax">
<p class="titleintable"><a id="sthref420"></a><a id="sthref421"></a>Table 6-1 Supported Storage Options<a id="sthref422"></a><a id="sthref423"></a><a id="sthref424"></a><a id="sthref425"></a> for Oracle Clusterware and Oracle RAC<a id="sthref426"></a><a id="sthref427"></a></p>
<table class="cellalignment4202" title="Supported Storage Options for Oracle Clusterware and Oracle RAC " summary="storage options for Oracle software and files" dir="ltr">
<thead>
<tr class="cellalignment4191">
<th class="cellalignment4203" id="r1c1-t3">Storage Option</th>
<th class="cellalignment4203" id="r1c2-t3">OCR and Voting Disks</th>
<th class="cellalignment4203" id="r1c3-t3">Oracle Clusterware binaries</th>
<th class="cellalignment4203" id="r1c4-t3">Oracle RAC binaries</th>
<th class="cellalignment4203" id="r1c5-t3">Oracle Database Files</th>
<th class="cellalignment4203" id="r1c6-t3">Oracle Recovery Files</th>
</tr>
</thead>
<tbody>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r2c1-t3" headers="r1c1-t3">
<p>Oracle Automatic Storage Management</p>
<p><span class="bold">Note</span>: Loopback devices are not supported for use with Oracle ASM</p>
</td>
<td class="cellalignment4197" headers="r2c1-t3 r1c2-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r2c1-t3 r1c3-t3">
<p>No</p>
</td>
<td class="cellalignment4197" headers="r2c1-t3 r1c4-t3">
<p>No</p>
</td>
<td class="cellalignment4197" headers="r2c1-t3 r1c5-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r2c1-t3 r1c6-t3">
<p>Yes</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r3c1-t3" headers="r1c1-t3">
<p>Oracle Automatic Storage Management Cluster File System (Oracle ACFS)</p>
</td>
<td class="cellalignment4197" headers="r3c1-t3 r1c2-t3">
<p>No</p>
</td>
<td class="cellalignment4197" headers="r3c1-t3 r1c3-t3">
<p>No</p>
</td>
<td class="cellalignment4197" headers="r3c1-t3 r1c4-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r3c1-t3 r1c5-t3">
<p>Yes for Oracle Database 12<span class="italic">c</span> Release 1 (12.1) and later</p>
</td>
<td class="cellalignment4197" headers="r3c1-t3 r1c6-t3">
<p>Yes for Oracle Database 12<span class="italic">c</span> Release 1 (12.1) and later</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r4c1-t3" headers="r1c1-t3">
<p>General Parallel File System (GPFS)<a id="sthref428"></a><a id="sthref429"></a><a id="sthref430"></a></p>
<ul>
<li>
<p><span class="bold">Note:</span> You cannot place ASM files on GPFS.</p>
</li>
<li>
<p>Oracle does not recommend the use of GPFS for voting disks if HACMP is used.</p>
</li>
</ul>
</td>
<td class="cellalignment4197" headers="r4c1-t3 r1c2-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r4c1-t3 r1c3-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r4c1-t3 r1c4-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r4c1-t3 r1c5-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r4c1-t3 r1c6-t3">
<p>Yes</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r5c1-t3" headers="r1c1-t3">
<p>Local file system</p>
</td>
<td class="cellalignment4197" headers="r5c1-t3 r1c2-t3">
<p>No</p>
</td>
<td class="cellalignment4197" headers="r5c1-t3 r1c3-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r5c1-t3 r1c4-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r5c1-t3 r1c5-t3">
<p>No</p>
</td>
<td class="cellalignment4197" headers="r5c1-t3 r1c6-t3">
<p>No</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r6c1-t3" headers="r1c1-t3">
<p>NFS file system on a certified NAS filer</p>
<p><span class="bold">Note:</span> Requires a certified NAS device. Oracle does not recommend the use of NFS for voting disks if HACMP is used.</p>
</td>
<td class="cellalignment4197" headers="r6c1-t3 r1c2-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r6c1-t3 r1c3-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r6c1-t3 r1c4-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r6c1-t3 r1c5-t3">
<p>Yes</p>
</td>
<td class="cellalignment4197" headers="r6c1-t3 r1c6-t3">
<p>Yes</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r7c1-t3" headers="r1c1-t3">
<p>Shared disk partitions (raw disks), including raw logical volumes managed by HACMP<a id="sthref431"></a><a id="sthref432"></a></p>
</td>
<td class="cellalignment4197" headers="r7c1-t3 r1c2-t3">
<p>Not supported by OUI or ASMCA, but supported by the software. They can be added or removed after installation.</p>
</td>
<td class="cellalignment4197" headers="r7c1-t3 r1c3-t3">
<p>No</p>
</td>
<td class="cellalignment4197" headers="r7c1-t3 r1c4-t3">
<p>No</p>
</td>
<td class="cellalignment4197" headers="r7c1-t3 r1c5-t3">
<p>Not supported by OUI or ASMCA, but supported by the software. They can be added or removed after installation.</p>
</td>
<td class="cellalignment4197" headers="r7c1-t3 r1c6-t3">
<p>No</p>
</td>
</tr>
</tbody>
</table>
<br/></div>
<!-- class="tblformalwidemax" -->
<p>Use the following guidelines when choosing storage options:</p>
<ul>
<li>
<p>You can choose any combination of the supported storage options for each file type provided that you satisfy all requirements listed for the chosen storage options.</p>
</li>
<li>
<p><a id="sthref433"></a>You can use Oracle ASM to store Oracle Clusterware files.</p>
</li>
<li>
<p>Direct use of r<a id="sthref434"></a><a id="sthref435"></a>aw or block devices is not supported.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a class="olink UPGRD001" href="../UPGRD/intro.htm#UPGRD001"><span class="italic">Oracle Database Upgrade Guide</span></a> for information about how to prepare for upgrading an existing database</div>
</li>
<li>
<p>If you do not have a storage option that provides external file redundancy, then you must configure at least three voting disk locations and at least two Oracle Cluster Registry locations to provide redundancy.</p>
</li>
</ul>
</div>
<!-- class="sect2" -->
<a id="BABCAJAI"></a>
<div id="CWAIX561" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.1.2</span> Oracle ACFS and Oracle ADVM</h3>
<p>This section contains information about Oracle Automatic Storage Management Cluster File System (Oracle ACFS) and Oracle Automatic Storage Management Dynamic Volume Manager (Oracle ADVM). It contains the following topics:</p>
<ul>
<li>
<p><a href="#BABJIGBG">About Oracle ACFS and Oracle ADVM</a></p>
</li>
<li>
<p><a href="#BABDAJJF">Restrictions and Guidelines for Oracle ACFS</a></p>
</li>
</ul>
<a id="BABJIGBG"></a>
<div id="CWAIX562" class="sect3">
<h4 class="sect3"><span class="secnum">6.1.2.1</span> About Oracle ACFS and Oracle ADVM</h4>
<p>Oracle ACFS extends Oracle ASM technology to support of all of your application data in both single instance and cluster configurations. Oracle ADVM provides volume management services and a standard disk device driver interface to clients. Oracle Automatic Storage Management Cluster File System communicates with Oracle ASM through the Oracle Automatic Storage Management Dynamic Volume Manager interface.</p>
</div>
<!-- class="sect3" -->
<a id="BABDAJJF"></a>
<div id="CWAIX423" class="sect3">
<h4 class="sect3"><span class="secnum">6.1.2.2</span> Restrictions and Guidelines for Oracle ACFS</h4>
<p>Note the following about Oracle ACFS:</p>
<ul>
<li>
<p>Oracle ACFS and Oracle ADVM are not supported on IBM AIX Workload Partitions (WPARs).</p>
</li>
<li>
<p>Oracle Automatic Storage Management Cluster File System (Oracle ACFS) provides a general purpose file system. You can place Oracle Database binaries and Oracle Database files on this system, but you cannot place Oracle Clusterware files on Oracle ACFS.</p>
<p><a id="sthref436"></a><a id="sthref437"></a>For policy-managed Oracle Flex Cluster databases, be aware that Oracle ACFS can run on Hub Nodes, but cannot run on Leaf Nodes. For this reason, Oracle RAC binaries cannot be placed on Oracle ACFS on Leaf Nodes.</p>
</li>
<li>
<p>For Oracle Flex Clusters, Leaf Nodes cannot mount an Oracle home on ACFS from the Hub Nodes; it is not supported for some nodes to access the same Oracle home using NFS while other nodes use ACFS for the same Oracle home path.</p>
</li>
<li>
<p>Oracle Restart does not support root-based Oracle Clusterware resources. For this reason, the following restrictions apply if you run Oracle ACFS on an Oracle Restart Configuration</p>
<ul>
<li>
<p>You must manually load and unload Oracle ACFS drivers.</p>
</li>
<li>
<p>You must manually mount and unmount Oracle ACFS file systems, after the Oracle ASM instance is running</p>
</li>
<li>
<p>With Oracle Restart no Oracle Grid Infrastructure resources can be defined for file systems. Therefore ACFS file systems cannot be used for database homes or data files.</p>
</li>
<li>
<p>You cannot place Oracle ACFS database home file systems into the Oracle ACFS mount registry. The mount registry is entirely removed with Oracle Grid Infrastructure 12<span class="italic">c</span> release 1.</p>
</li>
</ul>
</li>
<li>
<p>You cannot place Oracle Clusterware binaries and files on Oracle ACFS.</p>
</li>
<li>
<p>With Oracle Grid Infrastructure for a cluster, creating Oracle data files on an Oracle ACFS file system is supported from Oracle Database 12<span class="italic">c</span> Release 1.</p>
</li>
<li>
<p>You can place Oracle Database binaries and administrative files (for example, trace files) on Oracle ACFS.</p>
</li>
<li>
<p>Oracle ACFS does not support replication or encryption with Oracle Database data files, tablespace files, control files, and redo logs.</p>
</li>
</ul>
<div class="infobox-note">
<p class="notep1">Note:</p>
On AIX, Oracle ACFS has the following installation requirements:
<ul>
<li>
<p>The AIX version must be AIX 7.1, or on AIX 6.1 TL4 SP2, or later updates to AIX 6.1 on PPC64. Starting with Oracle Grid Infrastructure for a Cluster 11<span class="italic">g</span> Release 2 (11.2.0.3), Oracle ACFS is supported on all technical levels of AIX 7.1.</p>
</li>
<li>
<p>The system must be running in the RAC mode, which is the default.</p>
</li>
<li>
<p>The owner of the Oracle Grid Infrastructure installation must be a local user.</p>
</li>
</ul>
</div>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="CDEBIEHA"></a>
<div id="CWAIX244" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.1.3</span> General Storage Considerations for Oracle Grid Infrastructure and Oracle RAC</h3>
<p><a id="sthref438"></a><a id="sthref439"></a><a id="sthref440"></a><a id="sthref441"></a><a id="sthref442"></a>For all installations, you must choose the storage option to use for Oracle Grid Infrastructure (Oracle Clusterware and Oracle ASM), and Oracle Real Application Clusters databases (Oracle RAC). To enable automated backups during the installation, you must also choose the storage option to use for recovery files (the Fast Recovery Area). You do not have to use the same storage option for each file type.</p>
<div id="CWAIX245" class="sect3"><a id="sthref443"></a>
<h4 class="sect3"><span class="secnum">6.1.3.1</span> General Storage Considerations for Oracle Clusterware</h4>
<p>Oracle Clusterware voting disks are used to monitor cluster node status, and Oracle Cluster Registry (OCR) files contain configuration information about the cluster. You can place voting disks and OCR files either in an ASM diskgroup, or on a cluster file system or shared network file system. Storage must be shared; any node that does not have access to an absolute majority of voting disks (more than half) will be restarted.</p>
</div>
<!-- class="sect3" -->
<div id="CWAIX246" class="sect3"><a id="sthref444"></a>
<h4 class="sect3"><span class="secnum">6.1.3.2</span> General Storage Considerations for Oracle RAC</h4>
<p>For Standard Edition and Standard Edition 2 (SE2) Oracle RAC installations, Oracle ASM is the only supported storage option for database and recovery files. For all installations, Oracle recommends that you create at least two separate Oracle ASM disk groups: One for Oracle Database data files, and one for recovery files. Oracle recommends that you place the Oracle Database disk group and the recovery files disk group in separate failure groups.</p>
<p>If you do not use Oracle ASM, then Oracle recommends that you place the data files and the Fast Recovery Area in shared storage located outside of the Oracle home, in separate locations, so that a hardware failure does not affect availability.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<ul>
<li>
<p><a class="olink ADMQS092" href="../ADMQS/GUID-292CD2F6-ECDC-4F94-B750-97DD2D526B22.htm#ADMQS092"><span class="italic">Oracle Database 2 Day DBA</span></a> for more information about using a Fast Recovery Area</p>
</li>
<li>
<p><a class="olink OSTMG94058" href="../OSTMG/GUID-DBA4C79A-A5EB-451A-905A-992395137871.htm#OSTMG94058"><span class="italic">Oracle Automatic Storage Management Administrator&#39;s Guide</span></a> for information about failure groups and best practices for high availability and recovery</p>
</li>
</ul>
</div>
<p>Note the following additional guidelines for supported storage options:</p>
<ul>
<li>
<p>You can choose any combination of the supported storage options for each file type provided that you satisfy all requirements listed for the chosen storage options.</p>
</li>
<li>
<p>If you intend to use Oracle ASM with Oracle RAC, and you are configuring a new Oracle ASM instance, then your system must meet the following conditions:</p>
<ul>
<li>
<p>All nodes on the cluster have Oracle Clusterware and Oracle ASM 12<span class="italic">c</span> Release 1 (12.1) installed as part of an Oracle Grid Infrastructure for a cluster installation.</p>
</li>
<li>
<p>Any existing Oracle ASM instance on any node in the cluster is shut down.</p>
</li>
</ul>
</li>
<li>
<p>If you do not have a storage option that provides external file redundancy, then you must configure at least three voting disk areas to provide voting disk redundancy.</p>
</li>
</ul>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="CHDJBEDC"></a>
<div id="CWAIX424" class="sect2">
<h3 class="sect2"><span class="secnum">6.1.4</span> Guidelines for Using Oracle ASM Disk Groups for Storage</h3>
<p>During Oracle Grid Infrastructure installation, you can create one disk group. After the Oracle Grid Infrastructure installation, you can create additional disk groups using ASMCA, SQL*Plus, or ASMCMD. Note that with Oracle Database 11g release 2 (11.2) and later releases, Oracle Database Configuration Assistant (DBCA) does not have the functionality to create disk groups for Oracle ASM.</p>
<p>If you install Oracle Database or Oracle RAC after you install Oracle Grid Infrastructure, then you can either use the same disk group for database files, OCR, and voting disk files, or you can use different disk groups. If you create multiple disk groups before installing Oracle RAC or before creating a database, then you can decide to do one of the following:</p>
<ul>
<li>
<p>Place the data files in the same disk group as the Oracle Clusterware files.</p>
</li>
<li>
<p>Use the same Oracle ASM disk group for data files and recovery files.</p>
</li>
<li>
<p>Use different disk groups for each file type.</p>
</li>
</ul>
<p>If you create only one disk group for storage, then the OCR and voting disk files, database files, and recovery files are contained in the one disk group. If you create multiple disk groups for storage, then you can choose to place files in different disk groups.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
The Oracle ASM instance that manages the existing disk group should be running in the Grid home.</div>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<p><a class="olink OSTMG" href="../OSTMG/toc.htm"><span class="italic">Oracle Automatic Storage Management Administrator&#39;s Guide</span></a> for information about creating disk groups</p>
</div>
</div>
<!-- class="sect2" -->
<a id="CHDDEFIB"></a>
<div id="CWAIX425" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.1.5</span> Using Logical Volume Managers with Oracle Grid Infrastructure and Oracle RAC</h3>
<p>Oracle Grid Infrastructure and Oracle RAC only support cluster-aware volume managers. Some third-party volume managers are not cluster-aware, and so are not supported. To confirm that a volume manager you want to use is supported, click <span class="bold">Certifications</span> on My Oracle Support to determine if your volume manager is certified for Oracle RAC. My Oracle Support is available at the following URL:</p>
<pre dir="ltr"><a href="https://support.oracle.com">https://support.oracle.com</a>
</pre></div>
<!-- class="sect2" -->
<a id="CDEEFHJC"></a>
<div id="CWAIX249" class="sect2">
<h3 class="sect2"><span class="secnum">6.1.6</span> After You Have Selected Disk Storage Options</h3>
<p>When you have determined your disk storage options, configure shared storage:<a id="sthref445"></a><a id="sthref446"></a><a id="sthref447"></a></p>
<ul>
<li>
<p><span class="bold">To use a file system</span>, refer to <a href="#BABHCHIB">Section 6.2, &#34;About Shared File System Storage Configuration.&#34;</a></p>
</li>
<li>
<p><span class="bold">To use Oracle Automatic Storage Management</span>, refer to <a href="#CDEDIGAH">Section 6.4, &#34;Oracle Automatic Storage Management Storage Configuration.&#34;</a></p>
</li>
</ul>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="BABHCHIB"></a>
<div id="CWAIX250" class="sect1"><!-- infolevel="all" infotype="General" -->
<h2 class="sect1"><span class="secnum">6.2</span> About Shared File System Storage Configuration</h2>
<p>The installer suggests default locations for the Oracle Cluster Registry (OCR) and the Oracle Clusterware voting disk, based on the shared storage locations detected on the server. If you choose to create these files on a file system, then review the following sections to complete storage requirements for Oracle Clusterware files:</p>
<ul>
<li>
<p><a href="#CDEGHFHG">Guidelines for Using a Shared File System with Oracle Grid Infrastructure</a></p>
</li>
<li>
<p><a href="#CDEDCFFB">Deciding to Use a Cluster File System for Oracle Clusterware Files</a></p>
</li>
<li>
<p><a href="#CHDIDDBD">About Direct NFS Client and Data File Storage</a></p>
</li>
<li>
<p><a href="#CDEGFECB">Deciding to Use NFS for Data Files</a></p>
</li>
<li>
<p><a href="#CDECCABF">Configuring HACMP Multinode Disk Heartbeat (MNDHB)</a></p>
</li>
<li>
<p><a href="#CIHFHGFF">Configuring Raw Logical Volumes for Oracle Clusterware</a></p>
</li>
<li>
<p><a href="#CIHGBEDA">Configuring New Oracle Clusterware Volume Group Raw Logical Volumes</a></p>
</li>
<li>
<p><a href="#CDEHIDJA">Creating a Volume Group for Database Files</a></p>
</li>
<li>
<p><a href="#CIHDDFDG">Creating a Volume Group for Oracle Clusterware</a></p>
</li>
<li>
<p><a href="#CIHDHDBG">Importing the Volume Group on the Other Cluster Nodes</a></p>
</li>
<li>
<p><a href="#CIHGCJDD">Activating the Volume Group in Concurrent Mode on All Cluster Nodes</a></p>
</li>
<li>
<p><a href="#CDEIIIAI">Creating Directories for Oracle Clusterware Files on Shared File Systems</a></p>
</li>
<li>
<p><a href="#CDEIGEAH">Creating Directories for Oracle Database Files on Shared File Systems</a></p>
</li>
<li>
<p><a href="#CHDJBDGA">Disabling Direct NFS Client Oracle Disk Management Control of NFS</a></p>
</li>
</ul>
<a id="CDEGHFHG"></a>
<div id="CWAIX251" class="sect2">
<h3 class="sect2"><span class="secnum">6.2.1</span> Guidelines for Using a Shared File System with Oracle Grid Infrastructure</h3>
<p>To use a shared file system for Oracle Clusterware, Oracle ASM, and Oracle RAC, the file system must comply with the following requirements:</p>
<ul>
<li>
<p>To use an <a id="sthref448"></a>NFS file system, it must be on a supported NAS device. Log in to My Oracle Support at the following URL, and click <span class="bold">Certifications</span> to find the most current information about supported NAS devices:</p>
<p><code dir="ltr"><a href="https://support.oracle.com/">https://support.oracle.com/</a></code></p>
</li>
<li>
<p>If you choose to place your Oracle Cluster Registry (OCR) files on a shared file system, then Oracle recommends that you configure your shared file systems in one of the following ways:</p>
<ul>
<li>
<p>The disks used for the file system are on a highly available storage device, (for example, a RAID device).</p>
</li>
<li>
<p>At least two file systems are mounted, and use the features of Oracle Clusterware 12<span class="italic">c</span> Release 1 (12.1) to provide redundancy for the OCR.</p>
</li>
</ul>
</li>
<li>
<p>If you choose to place your database files on a shared file system, then one of the following should be true:</p>
<ul>
<li>
<p>The disks used for the file system are on a highly available storage device, (for example, a RAID<a id="sthref449"></a><a id="sthref450"></a><a id="sthref451"></a> device).</p>
</li>
<li>
<p>The file systems consist of at least two independent file systems, with the database files on one file system, and the recovery files on a different file system.</p>
</li>
</ul>
</li>
<li>
<p>The user account with which you perform the installation (<code dir="ltr">oracle</code> or <code dir="ltr">grid</code>) must have write permissions to create the files in the path that you specify.</p>
</li>
</ul>
<div class="infobox-note">
<p class="notep1">Note:</p>
Upgrading from Oracle9<span class="italic">i</span> Release 2 using the raw device or shared file for the OCR that you used for the SRVM configuration repository is not supported.
<p><a id="sthref452"></a><a id="sthref453"></a><a id="sthref454"></a><a id="sthref455"></a><a id="sthref456"></a><a id="sthref457"></a>If you are upgrading Oracle Clusterware, and your existing cluster uses 100 MB OCR and 20 MB voting disk partitions, then you must extend the OCR partition to at least 400 MB, and you should extend the voting disk partition to 300 MB. Oracle recommends that you do not use partitions, but instead place OCR and voting disks in disk groups marked as QUORUM disk groups.</p>
<p>All storage products must be supported by both your server and storage vendors.</p>
</div>
</div>
<!-- class="sect2" -->
<div id="CWAIX563" class="sect2"><!-- infolevel="all" infotype="General" --><a id="sthref458"></a>
<h3 class="sect2"><span class="secnum">6.2.2</span> Requirements for Oracle Grid Infrastructure Shared File System Volume Sizes</h3>
<p><a id="sthref459"></a>Use <a href="#BABBGIJE">Table 6-2</a> and <a href="#BABFFGGF">Table 6-3</a> to determine the minimum size for shared file systems:</p>
<div id="CWAIX564" class="tblformal">
<p class="titleintable"><a id="sthref460"></a><a id="BABBGIJE"></a>Table 6-2 Oracle Clusterware Shared File System Volume Size Requirements</p>
<table class="cellalignment4204" title="Oracle Clusterware Shared File System Volume Size Requirements" summary="Lists minimum partition sizes for shared file systems for Oracle Clusterware, including minimum sizes for database files if these are also stored on shared filesystem partitions" dir="ltr">
<thead>
<tr class="cellalignment4191">
<th class="cellalignment4203" id="r1c1-t10">File Types Stored</th>
<th class="cellalignment4203" id="r1c2-t10">Number of Volumes</th>
<th class="cellalignment4203" id="r1c3-t10">Volume Size</th>
</tr>
</thead>
<tbody>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r2c1-t10" headers="r1c1-t10">
<p>Voting disks with external redundancy</p>
</td>
<td class="cellalignment4197" headers="r2c1-t10 r1c2-t10">
<p>1</p>
</td>
<td class="cellalignment4197" headers="r2c1-t10 r1c3-t10">
<p>At least 300 MB for each voting disk volume</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r3c1-t10" headers="r1c1-t10">
<p>Oracle Cluster Registry (OCR) with external redundancy and the Grid Infrastructure Management Repository</p>
</td>
<td class="cellalignment4197" headers="r3c1-t10 r1c2-t10">
<p>1</p>
</td>
<td class="cellalignment4197" headers="r3c1-t10 r1c3-t10">
<p>At least 5.9 GB for the OCR volume that contains the Grid Infrastructure Management Repository(5.2 GB + 300 MB voting files + 400 MB OCR), plus 500 MB for each node for clusters greater than four nodes. For example, a six-node cluster allocation should be 6.9 GB.</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r4c1-t10" headers="r1c1-t10">
<p>Oracle Clusterware files (OCR and voting disks) and Grid Infrastructure Management Repository with redundancy provided by Oracle software</p>
</td>
<td class="cellalignment4197" headers="r4c1-t10 r1c2-t10">
<p>3</p>
</td>
<td class="cellalignment4197" headers="r4c1-t10 r1c3-t10">
<p>At least 400 MB for each OCR volume</p>
<p>At least 300 MB for each voting disk volume</p>
<p>2 x 5.2 GB (normal redundancy):</p>
<p>For 5 nodes and beyond, add 500 MB for each additional node.</p>
<p>For example, for a 6 node cluster the size is 14.1 GB:</p>
<ul>
<li>
<p>Grid Infrastructure Management Repository 2 x (5.2 GB+ 500 MB + 500 MB) = 12.4 GB</p>
</li>
<li>
<p>2 OCRs (2 x 400 MB) = 800 MB</p>
</li>
<li>
<p>3 voting files (3 x 300 MB) = 900 MB</p>
</li>
</ul>
<p>= 14.1 GB</p>
</td>
</tr>
</tbody>
</table>
<br/></div>
<!-- class="tblformal" -->
<div id="CWAIX565" class="tblformal">
<p class="titleintable"><a id="sthref461"></a><a id="BABFFGGF"></a>Table 6-3 Oracle RAC Shared File System Volume Size Requirements</p>
<table class="cellalignment4204" title="Oracle RAC Shared File System Volume Size Requirements" summary="Lists minimum partition sizes for shared file systems for Oracle Clusterware, including minimum sizes for database files if these are also stored on shared filesystem partitions" dir="ltr">
<thead>
<tr class="cellalignment4191">
<th class="cellalignment4203" id="r1c1-t11">File Types Stored</th>
<th class="cellalignment4203" id="r1c2-t11">Number of Volumes</th>
<th class="cellalignment4203" id="r1c3-t11">Volume Size</th>
</tr>
</thead>
<tbody>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r2c1-t11" headers="r1c1-t11">
<p>Oracle Database files</p>
</td>
<td class="cellalignment4197" headers="r2c1-t11 r1c2-t11">
<p>1</p>
</td>
<td class="cellalignment4197" headers="r2c1-t11 r1c3-t11">
<p>At least 1.5 GB for each volume</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r3c1-t11" headers="r1c1-t11">
<p>Recovery files</p>
<p><span class="bold">Note</span>: Recovery files must be on a different volume than database files</p>
</td>
<td class="cellalignment4197" headers="r3c1-t11 r1c2-t11">
<p>1</p>
</td>
<td class="cellalignment4197" headers="r3c1-t11 r1c3-t11">
<p>At least 2 GB for each volume</p>
</td>
</tr>
</tbody>
</table>
<br/></div>
<!-- class="tblformal" -->
<p>In <a href="#BABBGIJE">Table 6-2</a> and <a href="#BABFFGGF">Table 6-3</a>, the total required volume size is cumulative. For example, to store all Oracle Clusterware files on the shared file system with normal redundancy, you should have at least 2 GB of storage available over a minimum of three volumes (three separate volume locations for the OCR and two OCR mirrors, and one voting disk on each volume). You should have a minimum of three physical disks, each at least 500 MB, to ensure that voting disks and OCR files are on separate physical disks. If you add Oracle RAC using one volume for database files and one volume for recovery files, then you should have at least 3.5 GB available storage over two volumes, and at least 6.9 GB available total for all volumes.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you create partitions on shared partitions with <code dir="ltr">fdisk</code> by specifying a device size, such as <code dir="ltr">+400M</code>, then the actual device created may be smaller than the size requested, based on the cylinder geometry of the disk. This is due to current fdisk restrictions. Oracle recommends that you partition the entire disk that you allocate for use by Oracle ASM.</div>
</div>
<!-- class="sect2" -->
<a id="CDEDCFFB"></a>
<div id="CWAIX254" class="sect2">
<h3 class="sect2"><span class="secnum">6.2.3</span> Deciding to Use a Cluster File System for Oracle Clusterware Files</h3>
<p>For new installations, Oracle recommends that you use Oracle Automatic Storage Management (Oracle ASM) to store voting disk and OCR files.</p>
<p>If you are installing on IBM POWER, and you want to use a cluster file system, then you must use the IBM General Parallel File System (GPFS).</p>
</div>
<!-- class="sect2" -->
<a id="CHDIDDBD"></a>
<div id="CWAIX426" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.2.4</span> About Direct NFS Client and Data File Storage<a id="sthref462"></a><a id="sthref463"></a></h3>
<p>Direct NFS Client is an alternative to using kernel-managed NFS. This section contains the following information about Direct NFS Client:</p>
<ul>
<li>
<p><a href="#CHDFGDIH">About Direct NFS Client Storage</a></p>
</li>
<li>
<p><a href="#CHDDCCAD">Using the oranfstab File with Direct NFS Client</a></p>
</li>
<li>
<p><a href="#CHDECJHE">About Mounting NFS Storage Devices with Direct NFS Client</a></p>
</li>
<li>
<p><a href="#CHDDEHIB">Specifying Network Paths with the Oranfstab File</a></p>
</li>
</ul>
<a id="CHDFGDIH"></a>
<div id="CWAIX427" class="sect3">
<h4 class="sect3"><span class="secnum">6.2.4.1</span> About Direct NFS Client Storage</h4>
<p>With Oracle Database, instead of using the operating system kernel NFS client, you can configure Oracle Database to access NFS servers directly using an Oracle internal client called Direct NFS Client. Direct NFS Client supports NFSv3, NFSv4 and NFSv4.1 protocols (excluding the Parallel NFS extension) to access the NFS server.</p>
<p>To enable Oracle Database to use Direct NFS Client, the NFS file systems must be mounted and available over regular NFS mounts before you start installation. Direct NFS Client manages settings after installation. If Oracle Database cannot open an NFS server using Direct NFS Client, then Oracle Database uses the platform operating system kernel NFS client. You should still set the kernel mount options as a backup, but for normal operation, Direct NFS Client uses its own NFS client.</p>
<p>Direct NFS Client supports up to four network paths to the NFS server. Direct NFS Client performs load balancing across all specified paths. If a specified path fails, then Direct NFS Client reissues I/O commands over any remaining paths.</p>
<p>Some NFS file servers require NFS clients to connect using reserved ports. If your filer is running with reserved port checking, then you must disable reserved port checking for Direct NFS Client to operate. To disable reserved port checking, consult your NFS file server documentation.</p>
<p>For NFS servers that restrict port range, you can use the <code dir="ltr">insecure</code> option to enable clients other than <code dir="ltr">root</code> to connect to the NFS server. Alternatively, you can disable Direct NFS Client as described in <a href="#CHDJBDGA">Section 6.3.10, &#34;Disabling Direct NFS Client Oracle Disk Management Control of NFS&#34;</a>.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
Use NFS servers supported for Oracle RAC. See the following URL for support information:
<p><code dir="ltr"><a href="https://support.oracle.com">https://support.oracle.com</a></code></p>
</div>
</div>
<!-- class="sect3" -->
<div id="CWAIX566" class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref464"></a>
<h4 class="sect3"><span class="secnum">6.2.4.2</span> About Direct NFS Client Configuration</h4>
<p>Direct NFS Client uses either the configuration file <code dir="ltr">$ORACLE_HOME/dbs/oranfstab</code> or the operating system mount tab file <code dir="ltr">/etc/mtab</code> to find out what mount points are available. If <code dir="ltr">oranfstab</code> is not present, then by default Direct NFS Client servers mount entries found in <code dir="ltr">/etc/mtab</code>. No other configuration is required. You can use <code dir="ltr">oranfstab</code> to specify additional specific Oracle Database operations to use Direct NFS Client. For example, you can use <code dir="ltr">oranfstab</code> to specify additional paths for a mount point.</p>
<p>Direct NFS Client supports up to four network paths to the NFS server. Direct NFS Client performs load balancing across all specified paths. If a specified path fails, then Direct NFS Client reissues I/O commands over any remaining paths.</p>
</div>
<!-- class="sect3" -->
<a id="CHDDCCAD"></a>
<div id="CWAIX428" class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3"><span class="secnum">6.2.4.3</span> Using the oranfstab File with Direct NFS Client</h4>
<p>If you use Direct NFS Client, then you can choose to use a new file specific for Oracle data file management, <code dir="ltr">oranfstab</code>, to specify additional options specific for Oracle Database to Direct NFS Client. For example, you can use <code dir="ltr">oranfstab</code> to specify additional paths for a mount point. You can add the <code dir="ltr">oranfstab</code> file either to <code dir="ltr">/etc</code> or to <code dir="ltr">$ORACLE_HOME/dbs</code>.</p>
<p>With shared Oracle homes, when the <code dir="ltr">oranfstab</code> file is placed in <code dir="ltr">$ORACLE_HOME/dbs</code>, the entries in the file are specific to a single database. In this case, all nodes running an Oracle RAC database use the same <code dir="ltr">$ORACLE_HOME/dbs/oranfstab</code> file. In non-shared Oracle RAC installs, <code dir="ltr">oranfstab</code> must be replicated on all nodes.</p>
<p>When the <code dir="ltr">oranfstab</code> file is placed in <code dir="ltr">/etc</code>, it is globally available to all Oracle databases, and can contain mount points used by all Oracle databases running on nodes in the cluster, including standalone databases. However, on Oracle RAC systems, if the <code dir="ltr">oranfstab</code> file is placed in <code dir="ltr">/etc</code>, then you must replicate the file <code dir="ltr">/etc/oranfstab</code> file on all nodes, and keep each <code dir="ltr">/etc/oranfstab</code> file synchronized on all nodes, just as you must with the <code dir="ltr">/etc/fstab</code> file.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a href="#CHDECJHE">Section 6.2.4.4, &#34;About Mounting NFS Storage Devices with Direct NFS Client&#34;</a> for information about configuring <code dir="ltr">/etc/fstab</code></div>
<p>In all cases, mount points must be mounted by the kernel NFS system, even when they are being served using Direct NFS Client. Refer to your vendor documentation to complete operating system NFS configuration and mounting.</p>
<div class="infobox-note">
<p class="notep1"><a id="sthref465"></a><a id="sthref466"></a><a id="sthref467"></a>Caution:</p>
Direct NFS Client will not serve an NFS server with write size values (<code dir="ltr">wtmax</code>) less than 32768.</div>
</div>
<!-- class="sect3" -->
<a id="CHDECJHE"></a>
<div id="CWAIX429" class="sect3">
<h4 class="sect3"><span class="secnum">6.2.4.4</span> About Mounting NFS Storage Devices with Direct NFS Client</h4>
<p>Direct NFS Client determines mount point settings to NFS storage devices based on the configurations in <code dir="ltr">/etc/mtab</code>, which are changed with configuring the <code dir="ltr">/etc/fstab</code> file.</p>
<p>Direct NFS Client searches for mount entries in the following order:</p>
<ol>
<li>
<p><code dir="ltr">$ORACLE_HOME/dbs/oranfstab</code></p>
</li>
<li>
<p><code dir="ltr">/etc/oranfstab</code></p>
</li>
<li>
<p><code dir="ltr">/etc/mtab</code></p>
</li>
</ol>
<p>Direct NFS Client uses the first matching entry as the mount point.</p>
<p>Oracle Database requires that mount points be mounted by the kernel NFS system even when served through Direct NFS Client.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
You can have only one active Direct NFS Client implementation for each instance. Using Direct NFS Client on an instance will prevent another Direct NFS Client implementation.</div>
<p>If Oracle Database uses Direct NFS Client mount points configured using <code dir="ltr">oranfstab</code>, then it first verifies kernel NFS mounts by cross-checking entries in <code dir="ltr">oranfstab</code> with operating system NFS mount points. If a mismatch exists, then Direct NFS Client logs an informational message, and does not operate.</p>
<p>If Oracle Database cannot open an NFS server using Direct NFS Client, then Oracle Database uses the platform operating system kernel NFS client. In this case, the kernel NFS mount options must be set up as defined in <a href="#BABJADBH">Section 6.3.3, &#34;Checking NFS Mount and Buffer Size Parameters for Oracle RAC&#34;</a>. Additionally, an informational message is logged into the Oracle alert and trace files indicating that Direct NFS Client could not connect to an NFS server.</p>
<p><a href="#CDEFBCDD">Section 6.1.1, &#34;Supported Storage Options&#34;</a> lists the file types that are supported by Direct NFS Client.</p>
<p>The Oracle files resident on the NFS server that are served by Direct NFS Client are also accessible through the operating system kernel NFS client.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a class="olink OSTMG02150" href="../OSTMG/GUID-70B9562B-4A72-47A3-914C-10D0C3A7DE07.htm#OSTMG02150"><span class="italic">Oracle Automatic Storage Management Administrator&#39;s Guide</span></a> for guidelines to follow regarding managing Oracle database data files created with Direct NFS Client or kernel NFS</div>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="CDEGFECB"></a>
<div id="CWAIX255" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.2.5</span> Deciding to Use <a id="sthref468"></a><a id="sthref469"></a>NFS for Data Files</h3>
<p>Network-attached storage (NAS) systems use NFS to access data. You can store data files on a supported NFS system.</p>
<p>NFS file systems must be mounted and available over NFS mounts before you start installation. Refer to your vendor documentation to complete NFS configuration and mounting.</p>
<p>Be aware that the performance of Oracle software and databases stored on NAS devices depends on the performance of the network connection between the Oracle server and the NAS device.</p>
<p>For this reason, Oracle recommends that you connect the server to the NAS device using a private dedicated network connection, which should be Gigabit Ethernet or better.</p>
</div>
<!-- class="sect2" -->
<a id="CDECCABF"></a>
<div id="CWAIX257" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.2.6</span> Configuring HACMP Multinode Disk Heartbeat (MNDHB)</h3>
<p>This section contains the following topics:<a id="sthref470"></a></p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
My Oracle Support for additional information about HACMP deployment and HACMP certification</div>
<ul>
<li>
<p><a href="#CDEBFCBH">Overview of Requirements for Using HACMP with Oracle Clusterware</a></p>
</li>
<li>
<p><a href="#CDEGDJFG">Deploying HACMP and MDNDHB for Oracle Clusterware</a></p>
</li>
<li>
<p><a href="#CDEEIHCD">Upgrading an Existing Oracle Clusterware and HACMP Installation</a></p>
</li>
</ul>
<a id="CDEBFCBH"></a>
<div id="CWAIX258" class="sect3">
<h4 class="sect3"><span class="secnum">6.2.6.1</span> Overview of Requirements for Using HACMP with Oracle Clusterware</h4>
<p>You must define one Multi-node Disk Heartbeat (MNDHB) network for each Oracle Clusterware voting disk. Each MNDHB and voting disk pair must be located on a single hard disk, separate from the other pairs. You must also configure MNDHB so that the node is halted if access is lost to a quorum of the MNDHB networks in the enhanced concurrent volume group.</p>
<p>To reduce the likelihood of a cluster partition, IBM recommends that HACMP is deployed with multiple IP networks and at least one non-IP network. The non-IP networks can be implemented using RS232 or disk heart-beating. For systems using Oracle RAC and HACMP enhanced concurrent resources (enhanced concurrent logical volumes) for database storage, you must configure MNDHB networks.</p>
<p>Install, configure and have HACMP running before installing Oracle Clusterware. For an Oracle RAC configuration, do not use HACMP for IP failovers on the Oracle RAC network interfaces (public, VIP or private). These network interfaces should not be configured to use HACMP IP failover, as Oracle Clusterware manages VIP failovers for Oracle RAC. The Oracle RAC network interfaces are bound to individual nodes and Oracle RAC instances. Problems can occur with Oracle Clusterware if HACMP reconfigures IP addresses over different interfaces, or fails over addresses across nodes. You only can use HACMP for failover of IP address on Oracle RAC nodes if Oracle RAC does not use these addresses.</p>
</div>
<!-- class="sect3" -->
<a id="CDEGDJFG"></a>
<div id="CWAIX259" class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3"><span class="secnum">6.2.6.2</span> Deploying HACMP and MDNDHB for Oracle Clusterware</h4>
<p>Complete the following tasks, replacing each term in italics with the appropriate response for your system, or carrying out the action described and entering the appropriate response for your image:<a id="sthref471"></a></p>
<ol>
<li>
<p>Start HACMP.</p>
</li>
<li>
<p>Enter the following command to ensure that the HACMP <code dir="ltr">clcomdES</code> daemon is running:</p>
<pre dir="ltr"># lssrc -s clcomdES
</pre>
<p>If the daemon is not running, then start it using the following command:</p>
<pre dir="ltr"># startsrc &ndash;s clcomdES
</pre></li>
<li>
<p>Ensure that your versions of HACMP and AIX meet the system requirements listed in <a href="preaix.htm#BEHHEHJB">Section 3.6, &#34;Operating System Requirements for IBM AIX on POWER Systems (64-Bit)&#34;</a>.</p>
</li>
<li>
<p>Create HACMP cluster and add the Oracle Clusterware nodes. For example:</p>
<pre dir="ltr"># smitty cm_add_change_show_an_hacmp_cluster.dialog
* Cluster Name [<span class="italic">mycluster</span>] 
</pre></li>
<li>
<p>Create an HACMP cluster node for each Oracle Clusterware node. For example:</p>
<pre dir="ltr"># smitty cm_add_a_node_to_the_hacmp_cluster_dialog 
* Node Name [<span class="italic">mycluster_node1</span>]
Communication Path to Node [] 
</pre></li>
<li>
<p>Create HACMP Ethernet heartbeat networks. The HACMP configuration requires network definitions. Select NO for the IP address takeover for these networks, since they are used by Oracle Clusterware.</p>
<p>Create at least two network definitions: one for the Oracle public interface and a second one for the Oracle private (cluster interconnect) network. Additional Ethernet heartbeat networks can be added if desired.</p>
<p>For example:</p>
<pre dir="ltr"># smitty cm_add_a_network_to_the_hacmp_cluster_select 
- select ether network 
* Network Name [<span class="italic">my_network_name</span>] 
* Network Type <span class="italic">ether</span> 
* Netmask [<span class="italic">my.network.netmask.here</span>] 
* Enable IP Address Takeover via IP Aliases [No] 
IP Address Offset for Heart beating over IP Aliases [] 
</pre></li>
<li>
<p>For each of the networks added in the previous step, define all of the IP names for each Oracle Clusterware node associated with that network, including the public, private and VIP names for each Oracle Clusterware node. For example:</p>
<pre dir="ltr"># smitty cm_add_communication_interfaces_devices.select 
- select: Add Pre-defined Communication Interfaces and Devices / Communication Interfaces / desired network 
* IP Label/Address [<span class="italic">node_ip_address</span>] 
* Network Type <span class="italic">ether</span> 
* Network Name <span class="italic">some_network_name</span> 
* Node Name [<span class="italic">my_node_name</span>] 
Network Interface [] 
</pre></li>
<li>
<p>Create an HACMP resource group for the enhanced concurrent volume group resource with the following options:</p>
<pre dir="ltr"># smitty config_resource_group.dialog.custom 
* Resource Group Name [<span class="italic">my_resource_group_name</span>] 
* Participating Nodes (Default Node Priority) [<span class="italic">mynode1</span>,<span class="italic">mynode2</span>,<span class="italic">mynode3</span>] 
Startup Policy Online On All Available Nodes 
Fallover Policy Bring Offline (On Error Node Only) 
Fallback Policy Never Fallback 
</pre></li>
<li>
<p>Create an AIX enhanced concurrent volume group (Big VG, or Scalable VG) using either the command <code dir="ltr">smitty mkvg</code>, or using command lines. The VG must contain at least one hard disk for each voting disk. You must configure at least three voting disks.</p>
<p>In the following example, where you see <span class="italic">default</span>, accept the default response:</p>
<pre dir="ltr"># smitty _mksvg 
VOLUME GROUP name [<span class="italic">my_vg_name</span>] PP SIZE in MB 
* PHYSICAL VOLUME names [<span class="italic">mydisk1</span>,<span class="italic">mydisk2</span>,<span class="italic">mydisk3</span>] 
Force the creation of a volume group? no 
Activate volume group AUTOMATICALLY no at system restart? 
Volume Group MAJOR NUMBER [] 
Create VG Concurrent Capable? enhanced concurrent 
Max PPs per VG in kilobytes <span class="italic">default</span>
Max Logical Volumes <span class="italic">default</span>
</pre></li>
<li>
<p>Under &#34;Change/Show Resources for a Resource Group (standard)&#34;, add the concurrent volume group to the resource group added in the preceding steps.</p>
<p>For example:</p>
<pre dir="ltr"># smitty cm_change_show_resources_std_resource_group_menu_dmn.select 
- <span class="italic">select_resource_group_from_step_6</span>
Resource Group Name shared_storage 
Participating Nodes (Default Node Priority) <span class="italic">mynode1</span>,<span class="italic">mynode2</span>,<span class="italic">mynode3</span>
Startup Policy Online On All Available Nodes 
Fallover Policy Bring Offline (On Error Node Only) 
Fallback Policy Never Fallback 
Concurrent Volume Groups [<span class="italic">enter_VG_from_step_7</span>]
Use forced varyon of volume groups, if necessary false 
Application Servers [] 
</pre></li>
<li>
<p>Using the following command, ensure that one MNDHB network is defined for each Oracle Clusterware voting disk. Each MNDHB and voting disk pair must be collocated on a single hard disk, separate from the other pairs. The MNDHB network and Voting Disks exist on shared logical volumes in an enhanced concurrent logical volume managed by HACMP as an enhanced concurrent resource. For each of the hard disks in the VG created in step 6 on which you want to place a voting disk logical volume (LV), create a MNDHB LV.</p>
<pre dir="ltr"># smitty cl_add_mndhb_lv 
- <span class="italic">select_resource_group_defined_in_step_6</span>
* Physical Volume name <span class="italic">enter F4, then select a hard disk</span>
Logical Volume Name [] 
Logical Volume Label [] 
Volume Group name ccvg 
Resource Group Name shared_storage 
Network Name [n]
</pre>
<div class="infobox-note">
<p class="notep1">Note:</p>
When you define the LVs for the Oracle Clusterware voting disks, they should be defined on the same disks: one for each disk, as used in this step for the MNDHB LVs.</div>
</li>
<li>
<p>Configure MNDHB so that the node is halted if access is lost to a quorum of the MNDHB networks in the enhanced concurrent volume group. For example:</p>
<pre dir="ltr"># smitty cl_set_mndhb_response 
- <span class="italic">select_the_VG_created_in_step_7</span> 
On loss of access Halt the node 
Optional notification method [] 
Volume Group ccvg 
</pre></li>
<li>
<p>Verify and Synchronize HACMP configuration. For example:</p>
<pre dir="ltr"># smitty cm_initialization_and_standard_config_menu_dmn 
- select &#34;Verify and Synchronize HACMP Configuration&#34; 
</pre>
<p>Enter Yes if prompted: &#34;Would you like to import shared VG: ccvg, in resource group <span class="italic">my_resource_group</span> onto node: <span class="italic">mynode</span> to node: racha702 [Yes / No]:&#34;</p>
</li>
<li>
<p>Add the Add the HACMP cluster node IP names to the file <code dir="ltr">/usr/es/sbin/cluster/etc/rhosts</code>.</p>
</li>
</ol>
</div>
<!-- class="sect3" -->
<a id="CDEEIHCD"></a>
<div id="CWAIX260" class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3"><span class="secnum">6.2.6.3</span> Upgrading an Existing Oracle Clusterware and HACMP Installation</h4>
<p>Complete the following procedure:<a id="sthref472"></a></p>
<ol>
<li>
<p>Back up all databases, and back up the Oracle Cluster Registry (OCR)</p>
</li>
<li>
<p>Shut down on all nodes all Oracle RAC databases, all node applications, and Oracle Clusterware.</p>
</li>
<li>
<p>Enter the following command to disable Oracle Clusterware from starting when nodes are restarted:</p>
<pre dir="ltr"># crsctl disable crs
</pre></li>
<li>
<p>Shut down HACMP on all nodes.</p>
</li>
<li>
<p>Install HACMP APAR IZ01809, following the directions in the README included with that APAR.</p>
</li>
<li>
<p>Determine if the existing voting disk LVs are already on separate hard disks, and if each of these disks have sufficient space (at least 256 MB for the MNDHB LVs. If this is true, then create a MNDHB LV on each of the hard disks. If this is not true, then create new MNDHB LVs and new voting disk LVs, located on separate hard disks using the following command, responding to the sections in italics with the appropriate information for your system:</p>
<pre dir="ltr"># smitty cl_add_mndhb_lv 
- <span class="italic">Select_resource_group</span>
* Physical Volume name <span class="italic">Enter F4, then select disk</span> for the MNDHB and Voting Disk pair
Logical Volume Name [] 
Logical Volume Label [] 
Volume Group name ccvg 
Resource Group Name shared_storage 
Network Name [net_diskhbmulti_01] 
</pre></li>
<li>
<p>Verify and Synchronize HACMP configuration.</p>
</li>
<li>
<p>Start HACMP on all nodes.</p>
</li>
<li>
<p>If you added new LVs for voting disks in step 5, then replace each of the existing voting disks with the new ones.</p>
</li>
<li>
<p>Enter the following command to re-enable Oracle Clusterware:</p>
<pre dir="ltr"># crsctl enable crs
</pre></li>
<li>
<p>Start Oracle Clusterware on all nodes, and verify that all resources start correctly.</p>
</li>
</ol>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="CIHFHGFF"></a>
<div id="CWAIX261" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.2.7</span> Configuring Raw Logical Volumes for Oracle Clusterware</h3>
<div class="infobox-note">
<p class="notep1">Note:</p>
To use raw logical volumes for Oracle Clusterware, HACMP must be installed and configured on all cluster nodes.</div>
<p>This section describes how to configure raw logical volumes for Oracle Clusterware and database file storage. The procedures in this section describe how to create a new volume group that contains the logical volumes required for both types of files.</p>
<p>Before you continue, review the following guidelines which contain important information about using volume groups with this release of Oracle RAC:</p>
<ul>
<li>
<p>You must use concurrent-capable volume groups for Oracle Clusterware.</p>
</li>
<li>
<p>The Oracle Clusterware files require less than 560 MB of disk space, with external redundancy. To make efficient use of the disk space in a volume group, Oracle recommends that you use the same volume group for the logical volumes for both the Oracle Clusterware files and the database files.</p>
</li>
<li>
<p>If you are upgrading an existing Oracle9<span class="italic">i</span> release 2 Oracle RAC installation that uses raw logical volumes, then you can use the existing SRVM configuration repository logical volume for the OCR and create a new logical volume in the same volume group for the Oracle Clusterware voting disk. However, you must remove this volume group from the HACMP concurrent resource group that activates it before you install Oracle Clusterware.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
The HACMP documentation for information about removing a volume group from a concurrent resource group.</div>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you are upgrading a database, then you must also create a new logical volume for the SYSAUX tablespace. Refer to <a href="#CIHGBEDA">Section 6.2.8, &#34;Configuring New Oracle Clusterware Volume Group Raw Logical Volumes&#34;</a> for more information about the requirements for the Oracle Clusterware voting disk and SYSAUX logical volumes.</div>
</li>
<li>
<p>You must use a HACMP concurrent resource group to activate new or existing volume groups that contain only database files (not Oracle Clusterware files).</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
The HACMP documentation for information about adding a volume group to a new or existing concurrent resource group.</div>
</li>
<li>
<p>All volume groups that you intend to use for Oracle Clusterware must be activated in concurrent mode before you start the installation.</p>
</li>
<li>
<p>The procedures in this section describe how to create basic volumes groups and volumes. If you want to configure more complex volumes, (using mirroring, for example), then use this section in conjunction with the HACMP documentation.</p>
</li>
</ul>
</div>
<!-- class="sect2" -->
<a id="CIHGBEDA"></a>
<div id="CWAIX262" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.2.8</span> Configuring New Oracle Clusterware Volume Group Raw Logical Volumes<a id="sthref473"></a><a id="sthref474"></a><a id="sthref475"></a></h3>
<p>To create the required raw logical volumes in the new Oracle Clusterware volume group:</p>
<ol>
<li>
<p>Identify the logical volumes that you must create.<a id="sthref476"></a><a id="sthref477"></a></p>
</li>
<li>
<p>If you prefer, you can also use the command <code dir="ltr">smit mklv</code> to create raw logical volumes.</p>
<p>The following example shows the command used to create a logical volume for the <code dir="ltr">ocr</code> volume group in the SYSAUX tablespace with a physical partition size of 114 MB (1792/7 = 256):</p>
<pre dir="ltr"># /usr/sbin/mklv -y test_sysaux_raw_1792m -T O -w n -s n -r n ocr 7
</pre></li>
<li>
<p>Change the owner, group, and permissions on the character device files associated with the logical volumes that you created, as follows:<a id="sthref478"></a><a id="sthref479"></a><a id="sthref480"></a><a id="sthref481"></a><a id="sthref482"></a><a id="sthref483"></a></p>
<div class="infobox-note">
<p class="notep1">Note:</p>
The device file associated with the Oracle Cluster Registry must be owned by <code dir="ltr">root</code>. All other device files must be owned by the Oracle software owner user (<code dir="ltr">oracle</code>).</div>
<pre dir="ltr"># chown oracle:dba /dev/rora_vote_raw_280m
# chmod 660 /dev/rora_vote_raw_280m
# chown root:oinstall /dev/rora_ocr_raw_280m
# chmod 640 /dev/rora_ocr_raw_280m
</pre></li>
</ol>
</div>
<!-- class="sect2" -->
<a id="CDEHIDJA"></a>
<div id="CWAIX263" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.2.9</span> Creating a Volume Group for Database Files<a id="sthref484"></a><a id="sthref485"></a><a id="sthref486"></a></h3>
<p>To create a volume group for the Oracle Database files:</p>
<ol>
<li>
<p>If necessary, install the shared disks that you intend to use.<a id="sthref487"></a></p>
</li>
<li>
<p>To ensure that the disks are available, enter the following command on every node: <a id="sthref488"></a><a id="sthref489"></a><a id="sthref490"></a><a id="sthref491"></a><a id="sthref492"></a></p>
<pre dir="ltr"># /usr/sbin/lsdev -Cc disk
</pre>
<p>The output from this command is similar to the following:<a id="sthref493"></a><a id="sthref494"></a><a id="sthref495"></a></p>
<pre dir="ltr">hdisk0 Available 1A-09-00-8,0  16 Bit LVD SCSI Disk Drive
hdisk1 Available 1A-09-00-9,0  16 Bit LVD SCSI Disk Drive
hdisk2 Available 17-08-L       SSA Logical Disk Drive
</pre></li>
<li>
<p>If a disk is not listed as available on any node, then enter the following command to configure the new disks:<a id="sthref496"></a><a id="sthref497"></a><a id="sthref498"></a><a id="sthref499"></a></p>
<pre dir="ltr"># /usr/sbin/cfgmgr
</pre></li>
<li>
<p>Enter the following command on any node to identify the device names and any associated volume group for each disk:<a id="sthref500"></a><a id="sthref501"></a><a id="sthref502"></a><a id="sthref503"></a><a id="sthref504"></a></p>
<pre dir="ltr"># /usr/sbin/lspv
</pre>
<p>The output from this command is similar to the following:</p>
<pre dir="ltr">hdisk0     0000078752249812   rootvg
hdisk1     none               none
hdisk4     00034b6fd4ac1d71   ccvg1
</pre>
<p>For each disk, this command shows:</p>
<ul>
<li>
<p>The disk device name</p>
</li>
<li>
<p>Either the 16 character physical volume identifier (PVID) if the disk has one, or <code dir="ltr">none</code></p>
</li>
<li>
<p>Either the volume group to which the disk belongs, or <code dir="ltr">none</code></p>
</li>
</ul>
<p>The disks that you want to use may have a PVID, but they must not belong to existing volume groups.</p>
</li>
<li>
<p>If a disk that you want to use for the volume group does not have a PVID, then enter a command similar to the following to assign one to it:<a id="sthref505"></a><a id="sthref506"></a><a id="sthref507"></a><a id="sthref508"></a><a id="sthref509"></a></p>
<pre dir="ltr"># /usr/sbin/chdev -l hdisk<span class="italic">n</span> -a pv=yes
</pre></li>
<li>
<p>To identify used device major numbers, enter the following command on each node of the cluster:<a id="sthref510"></a><a id="sthref511"></a><a id="sthref512"></a></p>
<pre dir="ltr"># ls -la /dev | more
</pre>
<p>This command displays information about all configured devices, similar to the following:</p>
<pre dir="ltr">crw-rw----   1 root     system    45,  0 Jul 19 11:56 vg1
</pre>
<p>In this example, 45 is the major number of the <code dir="ltr">vg1</code> volume group device.</p>
</li>
<li>
<p>Identify an appropriate major number that is unused on all nodes in the cluster.</p>
</li>
<li>
<p>To create a volume group, enter a command similar to the following, or use SMIT (<code dir="ltr">smit mkvg</code>):<a id="sthref513"></a><a id="sthref514"></a><a id="sthref515"></a><a id="sthref516"></a><a id="sthref517"></a></p>
<pre dir="ltr"># /usr/sbin/mkvg -y <span class="italic">VGname</span> -B -s <span class="italic">PPsize</span> -V <span class="italic">majornum</span> -n \
-C <span class="italic">PhysicalVolumes</span>
</pre></li>
<li>
<p>The following table describes the options and variables used in this example. Refer to the <code dir="ltr">mkvg</code> man page for more information about these options.</p>
<div class="inftblhruleinformal">
<table class="cellalignment4202" title="Options and Variables Used in the mkvg Command" summary="This table describes the options and variables used in the mkvg command example" dir="ltr">
<thead>
<tr class="cellalignment4191">
<th class="cellalignment4203" id="r1c1-t25">Command Option</th>
<th class="cellalignment4203" id="r1c2-t25">SMIT Field</th>
<th class="cellalignment4203" id="r1c3-t25">Sample Value and Description</th>
</tr>
</thead>
<tbody>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r2c1-t25" headers="r1c1-t25">
<pre dir="ltr">-y <span class="italic">VGname</span>
</pre></td>
<td class="cellalignment4197" headers="r2c1-t25 r1c2-t25"><span class="bold">VOLUME GROUP name</span></td>
<td class="cellalignment4197" headers="r2c1-t25 r1c3-t25">
<pre dir="ltr">oracle_vg1
</pre>
Specify the name for the volume group. The name that you specify could be a generic name, as shown, or it could specify the name of the database that you intend to create.</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r3c1-t25" headers="r1c1-t25">
<pre dir="ltr">-y <span class="italic">VGname</span>
</pre></td>
<td class="cellalignment4197" headers="r3c1-t25 r1c2-t25"><span class="bold">VOLUME GROUP name</span></td>
<td class="cellalignment4197" headers="r3c1-t25 r1c3-t25">
<pre dir="ltr">oracle_vg1
</pre>
Specify the name for the volume group. The name that you specify could be a generic name, as shown, or for a database volume group, it could specify the name of the database that you intend to create.</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r4c1-t25" headers="r1c1-t25">
<pre dir="ltr">-B
</pre></td>
<td class="cellalignment4197" headers="r4c1-t25 r1c2-t25"><span class="bold">Create a big VG format Volume Group</span></td>
<td class="cellalignment4197" headers="r4c1-t25 r1c3-t25">Specify this option to create a big VG format volume group.
<p><span class="bold">Note:</span> If you are using SMIT, then choose <span class="bold">yes</span> for this field.</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r5c1-t25" headers="r1c1-t25">
<pre dir="ltr">-s <span class="italic">PPsize</span>
</pre></td>
<td class="cellalignment4197" headers="r5c1-t25 r1c2-t25"><span class="bold">Physical partition SIZE in megabytes</span></td>
<td class="cellalignment4197" headers="r5c1-t25 r1c3-t25">
<pre dir="ltr">32
</pre>
Specify the size of the physical partitions for the database. The sample value shown enables you to include a disk up to 32 GB in size (32 MB * 1016).</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r6c1-t25" headers="r1c1-t25">
<pre dir="ltr">-V <span class="italic">Majornum</span>
</pre></td>
<td class="cellalignment4197" headers="r6c1-t25 r1c2-t25"><span class="bold">Volume Group MAJOR NUMBER</span></td>
<td class="cellalignment4197" headers="r6c1-t25 r1c3-t25">
<pre dir="ltr">46
</pre>
Specify the device major number for the volume group that you identified in Step 7.</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r7c1-t25" headers="r1c1-t25">
<pre dir="ltr">-n
</pre></td>
<td class="cellalignment4197" headers="r7c1-t25 r1c2-t25"><span class="bold">Activate volume group AUTOMATICALLY at system restart</span></td>
<td class="cellalignment4197" headers="r7c1-t25 r1c3-t25">Specify this option to prevent the volume group from being activated at system restart.
<p><span class="bold">Note:</span> If you are using SMIT, then choose <span class="bold">no</span> for this field.</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r8c1-t25" headers="r1c1-t25">
<pre dir="ltr">-C
</pre></td>
<td class="cellalignment4197" headers="r8c1-t25 r1c2-t25"><span class="bold">Create VG Concurrent Capable</span></td>
<td class="cellalignment4197" headers="r8c1-t25 r1c3-t25">Specify this option to create a concurrent capable volume group.
<p><span class="bold">Note:</span> If you are using SMIT, then choose <span class="bold">yes</span> for this field.</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r9c1-t25" headers="r1c1-t25">
<pre dir="ltr"><span class="italic">PhysicalVolumes</span>
</pre></td>
<td class="cellalignment4197" headers="r9c1-t25 r1c2-t25"><span class="bold">PHYSICAL VOLUME names</span></td>
<td class="cellalignment4197" headers="r9c1-t25 r1c3-t25">
<pre dir="ltr">hdisk3 hdisk4
</pre>
Specify the device names of the disks that you want to add to the volume group.</td>
</tr>
</tbody>
</table>
<br/></div>
<!-- class="inftblhruleinformal" --></li>
<li>
<p>Enter a command similar to the following to vary on the volume group that you created:<a id="sthref518"></a><a id="sthref519"></a></p>
<pre dir="ltr"># /usr/sbin/varyonvg <span class="italic">VGname</span>
</pre></li>
</ol>
</div>
<!-- class="sect2" -->
<a id="CIHDDFDG"></a>
<div id="CWAIX264" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.2.10</span> Creating a Volume Group for Oracle Clusterware<a id="sthref520"></a><a id="sthref521"></a><a id="sthref522"></a></h3>
<p>To create a volume group for the Oracle Clusterware files:</p>
<ol>
<li>
<p>If necessary, install the shared disks that you intend to use.<a id="sthref523"></a></p>
</li>
<li>
<p>To ensure that the disks are available, enter the following command on every node: <a id="sthref524"></a><a id="sthref525"></a><a id="sthref526"></a><a id="sthref527"></a><a id="sthref528"></a></p>
<pre dir="ltr"># /usr/sbin/lsdev -Cc disk
</pre>
<p>The output from this command is similar to the following:<a id="sthref529"></a><a id="sthref530"></a><a id="sthref531"></a></p>
<pre dir="ltr">hdisk0 Available 1A-09-00-8,0  16 Bit LVD SCSI Disk Drive
hdisk1 Available 1A-09-00-9,0  16 Bit LVD SCSI Disk Drive
hdisk2 Available 17-08-L       SSA Logical Disk Drive
</pre></li>
<li>
<p>If a disk is not listed as available on any node, then enter the following command to configure the new disks:<a id="sthref532"></a><a id="sthref533"></a><a id="sthref534"></a><a id="sthref535"></a></p>
<pre dir="ltr"># /usr/sbin/cfgmgr
</pre></li>
<li>
<p>Enter the following command on any node to identify the device names and any associated volume group for each disk:<a id="sthref536"></a><a id="sthref537"></a><a id="sthref538"></a><a id="sthref539"></a><a id="sthref540"></a></p>
<pre dir="ltr"># /usr/sbin/lspv
</pre>
<p>The output from this command is similar to the following:</p>
<pre dir="ltr">hdisk0     0000078752249812   rootvg
hdisk1     none               none
hdisk4     00034b6fd4ac1d71   ccvg1
</pre>
<p>For each disk, this command shows:</p>
<ul>
<li>
<p>The disk device name</p>
</li>
<li>
<p>Either the 16 character physical volume identifier (PVID) if the disk has one, or <code dir="ltr">none</code></p>
</li>
<li>
<p>Either the volume group to which the disk belongs, or <code dir="ltr">none</code></p>
</li>
</ul>
<p>The disks that you want to use may have a PVID, but they must not belong to existing volume groups.</p>
</li>
<li>
<p>If a disk that you want to use for the volume group does not have a PVID, then enter a command similar to the following to assign one to it:<a id="sthref541"></a><a id="sthref542"></a><a id="sthref543"></a><a id="sthref544"></a><a id="sthref545"></a></p>
<pre dir="ltr"># /usr/sbin/chdev -l hdisk<span class="italic">n</span> -a pv=yes
</pre></li>
<li>
<p>To identify used device major numbers, enter the following command on each node of the cluster:<a id="sthref546"></a><a id="sthref547"></a><a id="sthref548"></a></p>
<pre dir="ltr"># ls -la /dev | more
</pre>
<p>This command displays information about all configured devices, similar to the following:</p>
<pre dir="ltr">crw-rw----   1 root     system    45,  0 Jul 19 11:56 vg1
</pre>
<p>In this example, 45 is the major number of the <code dir="ltr">vg1</code> volume group device.</p>
</li>
<li>
<p>Identify an appropriate major number that is unused on all nodes in the cluster.</p>
</li>
<li>
<p>To create a volume group, enter a command similar to the following, or use SMIT (<code dir="ltr">smit mkvg</code>):<a id="sthref549"></a><a id="sthref550"></a><a id="sthref551"></a><a id="sthref552"></a><a id="sthref553"></a></p>
<pre dir="ltr"># /usr/sbin/mkvg -y <span class="italic">VGname</span> -B -s <span class="italic">PPsize</span> -V <span class="italic">majornum</span> -n \
-C <span class="italic">PhysicalVolumes</span>
</pre></li>
<li>
<p>The following table describes the options and variables used in this example. Refer to the <code dir="ltr">mkvg</code> man page for more information about these options.</p>
<div class="inftblhruleinformal">
<table class="cellalignment4202" title="Options and Variables Used in the mkvg Command" summary="This table describes the options and variables used in the mkvg command example" dir="ltr">
<thead>
<tr class="cellalignment4191">
<th class="cellalignment4203" id="r1c1-t26">Command Option</th>
<th class="cellalignment4203" id="r1c2-t26">SMIT Field</th>
<th class="cellalignment4203" id="r1c3-t26">Sample Value and Description</th>
</tr>
</thead>
<tbody>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r2c1-t26" headers="r1c1-t26">
<pre dir="ltr">-y <span class="italic">VGname</span>
</pre></td>
<td class="cellalignment4197" headers="r2c1-t26 r1c2-t26"><span class="bold">VOLUME GROUP name</span></td>
<td class="cellalignment4197" headers="r2c1-t26 r1c3-t26">
<pre dir="ltr">oracle_vg1
</pre>
Specify the name for the volume group. The name that you specify could be a generic name, as shown, or it could specify the name of the database that you intend to create.</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r3c1-t26" headers="r1c1-t26">
<pre dir="ltr">-y <span class="italic">VGname</span>
</pre></td>
<td class="cellalignment4197" headers="r3c1-t26 r1c2-t26"><span class="bold">VOLUME GROUP name</span></td>
<td class="cellalignment4197" headers="r3c1-t26 r1c3-t26">
<pre dir="ltr">oracle_vg1
</pre>
Specify the name for the volume group. The name that you specify could be a generic name, as shown, or for a database volume group, it could specify the name of the database that you intend to create.</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r4c1-t26" headers="r1c1-t26">
<pre dir="ltr">-B
</pre></td>
<td class="cellalignment4197" headers="r4c1-t26 r1c2-t26"><span class="bold">Create a big VG format Volume Group</span></td>
<td class="cellalignment4197" headers="r4c1-t26 r1c3-t26">Specify this option to create a big VG format volume group.
<p><span class="bold">Note:</span> If you are using SMIT, then choose <span class="bold">yes</span> for this field.</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r5c1-t26" headers="r1c1-t26">
<pre dir="ltr">-s <span class="italic">PPsize</span>
</pre></td>
<td class="cellalignment4197" headers="r5c1-t26 r1c2-t26"><span class="bold">Physical partition SIZE in megabytes</span></td>
<td class="cellalignment4197" headers="r5c1-t26 r1c3-t26">
<pre dir="ltr">32
</pre>
Specify the size of the physical partitions for the database. The sample value shown enables you to include a disk up to 32 GB in size (32 MB * 1016).</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r6c1-t26" headers="r1c1-t26">
<pre dir="ltr">-V <span class="italic">Majornum</span>
</pre></td>
<td class="cellalignment4197" headers="r6c1-t26 r1c2-t26"><span class="bold">Volume Group MAJOR NUMBER</span></td>
<td class="cellalignment4197" headers="r6c1-t26 r1c3-t26">
<pre dir="ltr">46
</pre>
Specify the device major number for the volume group that you identified in Step 7.</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r7c1-t26" headers="r1c1-t26">
<pre dir="ltr">-n
</pre></td>
<td class="cellalignment4197" headers="r7c1-t26 r1c2-t26"><span class="bold">Activate volume group AUTOMATICALLY at system restart</span></td>
<td class="cellalignment4197" headers="r7c1-t26 r1c3-t26">Specify this option to prevent the volume group from being activated at system restart.
<p><span class="bold">Note:</span> If you are using SMIT, then choose <span class="bold">no</span> for this field.</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r8c1-t26" headers="r1c1-t26">
<pre dir="ltr">-C
</pre></td>
<td class="cellalignment4197" headers="r8c1-t26 r1c2-t26"><span class="bold">Create VG Concurrent Capable</span></td>
<td class="cellalignment4197" headers="r8c1-t26 r1c3-t26">Specify this option to create a concurrent capable volume group.
<p><span class="bold">Note:</span> If you are using SMIT, then choose <span class="bold">yes</span> for this field.</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r9c1-t26" headers="r1c1-t26">
<pre dir="ltr"><span class="italic">PhysicalVolumes</span>
</pre></td>
<td class="cellalignment4197" headers="r9c1-t26 r1c2-t26"><span class="bold">PHYSICAL VOLUME names</span></td>
<td class="cellalignment4197" headers="r9c1-t26 r1c3-t26">
<pre dir="ltr">hdisk3 hdisk4
</pre>
Specify the device names of the disks that you want to add to the volume group.</td>
</tr>
</tbody>
</table>
<br/></div>
<!-- class="inftblhruleinformal" --></li>
<li>
<p>Enter a command similar to the following to vary on the volume group that you created:<a id="sthref554"></a><a id="sthref555"></a></p>
<pre dir="ltr"># /usr/sbin/varyonvg <span class="italic">VGname</span>
</pre></li>
</ol>
</div>
<!-- class="sect2" -->
<a id="CIHDHDBG"></a>
<div id="CWAIX265" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.2.11</span> Importing the Volume Group on the Other Cluster Nodes<a id="sthref556"></a><a id="sthref557"></a><a id="sthref558"></a></h3>
<p>To make the volume group available to all nodes in the cluster, you must import it on each node, as follows:</p>
<ol>
<li>
<p>Because the physical volume names may be different on the other nodes, enter the following command to determine the PVID of the physical volumes used by the volume group:<a id="sthref559"></a><a id="sthref560"></a></p>
<pre dir="ltr"># /usr/sbin/lspv
</pre></li>
<li>
<p>Note the PVIDs of the physical devices used by the volume group.</p>
</li>
<li>
<p>To vary off the volume group that you want to use, enter a command similar to the following on the node where you created it:<a id="sthref561"></a><a id="sthref562"></a></p>
<pre dir="ltr"># /usr/sbin/varyoffvg <span class="italic">VGname</span>
</pre></li>
<li>
<p>On each cluster node, complete the following steps:</p>
<ol>
<li>
<p>Enter the following command to determine the physical volume names associated with the PVIDs you noted previously:<a id="sthref563"></a><a id="sthref564"></a></p>
<pre dir="ltr"># /usr/sbin/lspv
</pre></li>
<li>
<p>On each node of the cluster, enter commands similar to the following to import the volume group definitions:<a id="sthref565"></a><a id="sthref566"></a></p>
<pre dir="ltr"># /usr/sbin/importvg -y <span class="italic">VGname</span> -V <span class="italic">MajorNumber</span> <span class="italic">PhysicalVolume</span>
</pre>
<p>In this example, <code dir="ltr"><span class="codeinlineitalic">MajorNumber</span></code> is the device major number for the volume group and <code dir="ltr"><span class="codeinlineitalic">PhysicalVolume</span></code> is the name of one of the physical volumes in the volume group.</p>
<p>For example, to import the definition of the <code dir="ltr">oracle_vg1</code> volume group with device major number 45 on the <code dir="ltr">hdisk3</code> and <code dir="ltr">hdisk4</code> physical volumes, enter the following command:<a id="sthref567"></a><a id="sthref568"></a></p>
<pre dir="ltr"># /usr/sbin/importvg -y oracle_vg1 -V 45 hdisk3
</pre></li>
<li>
<p>Change the owner, group, and permissions on the character device files associated with the logical volumes you created, as follows:<a id="sthref569"></a><a id="sthref570"></a><a id="sthref571"></a><a id="sthref572"></a><a id="sthref573"></a><a id="sthref574"></a></p>
<pre dir="ltr"># chown oracle:dba /dev/rora_vote_raw_280m
# chmod 660 /dev/rora_vote_raw_280m
# chown root:oinstall /dev/rora_ocr_raw_280m
# chmod 640 /dev/rora_ocr_raw_280m
</pre></li>
<li>
<p>Enter the following command to ensure that the volume group will not be activated by the operating system when the node starts:</p>
<pre dir="ltr"># /usr/sbin/chvg -a n <span class="italic">VGname</span>
</pre></li>
</ol>
</li>
</ol>
</div>
<!-- class="sect2" -->
<a id="CIHGCJDD"></a>
<div id="CWAIX266" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.2.12</span> Activating the Volume Group in Concurrent Mode on All Cluster Nodes<a id="sthref575"></a><a id="sthref576"></a><a id="sthref577"></a></h3>
<p>To activate the volume group in concurrent mode on all cluster nodes, enter the following command on each node:</p>
<pre dir="ltr"># /usr/sbin/varyonvg -c <span class="italic">VGname</span>
</pre></div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="BABCCCID"></a>
<div id="CWAIX567" class="sect1"><!-- infolevel="all" infotype="General" -->
<h2 class="sect1"><span class="secnum">6.3</span> Configuring Operating System and Direct NFS Client</h2>
<p>Refer to the following sections to configure your operating system and Direct NFS Client:</p>
<ul>
<li>
<p><a href="#BABCHGHA">Configuring Operating System NFS Mount and Buffer Size Parameters</a></p>
</li>
<li>
<p><a href="#BABHHDDI">Checking Operating System NFS Mount and Buffer Size Parameters</a></p>
</li>
<li>
<p><a href="#BABJADBH">Checking NFS Mount and Buffer Size Parameters for Oracle RAC</a></p>
</li>
<li>
<p><a href="#BABFIEDI">Setting TCP Network Protocol Buffer for Direct NFS Client</a></p>
</li>
<li>
<p><a href="#CHDIJGCA">Enabling Direct NFS Client Oracle Disk Manager Control of NFS</a></p>
</li>
<li>
<p><a href="#CHDDEHIB">Specifying Network Paths with the Oranfstab File</a></p>
</li>
<li>
<p><a href="#BABJBEEG">Enabling Hybrid Columnar Compression on Direct NFS Client</a></p>
</li>
<li>
<p><a href="#CDEIIIAI">Creating Directories for Oracle Clusterware Files on Shared File Systems</a></p>
</li>
<li>
<p><a href="#CDEIGEAH">Creating Directories for Oracle Database Files on Shared File Systems</a></p>
</li>
<li>
<p><a href="#CHDJBDGA">Disabling Direct NFS Client Oracle Disk Management Control of NFS</a></p>
</li>
</ul>
<a id="BABCHGHA"></a>
<div id="CWAIX568" class="sect2">
<h3 class="sect2"><span class="secnum">6.3.1</span> Configuring Operating System <a id="sthref578"></a>NFS Mount and Buffer Size Parameters</h3>
<p>If you are using NFS for the Grid home or Oracle RAC home, then you must set up the NFS mounts on the storage to enable the following:</p>
<ul>
<li>
<p>The <code dir="ltr">root</code> user on the clients mounting to the storage can be considered as the <code dir="ltr">root</code> user on the file server, instead of being mapped to an anonymous user.</p>
</li>
<li>
<p>The <code dir="ltr">root</code> user on the client server can create files on the NFS filesystem that are owned by <code dir="ltr">root</code> on the file server.</p>
</li>
</ul>
<p>On NFS, you can obtain <code dir="ltr">root</code> access for clients writing to the storage by enabling <code dir="ltr">no_root_squash</code> on the server side. For example, to set up Oracle Clusterware file storage in the path <code dir="ltr">/vol/grid</code>, with nodes node1, node 2, and node3 in the domain <code dir="ltr">mycluster.example.com</code>, add a line similar to the following to the <code dir="ltr">/etc/exports</code> file:</p>
<pre dir="ltr">/vol/grid/ node1.mycluster.example.com(rw,no_root_squash)
node2.mycluster.example.com(rw,no_root_squash) node3.mycluster.example.com
(rw,no_root_squash) 
</pre>
<p>If the domain or DNS is secure so that no unauthorized system can obtain an IP address on it, then you can grant <code dir="ltr">root</code> access by domain, rather than specifying particular cluster member nodes:</p>
<p>For example:</p>
<pre dir="ltr">/vol/grid/ *.mycluster.example.com(rw,no_root_squash)
</pre>
<p>Oracle recommends that you use a secure DNS or domain, and grant <code dir="ltr">root</code> access to cluster member nodes using the domain, because using this syntax enables you to add or remove nodes without the need to reconfigure the NFS server.</p>
<p>If you use Grid Naming Service (GNS), then the subdomain allocated for resolution by GNS within the cluster is a secure domain. Any server without a correctly signed Grid Plug and Play (GPnP) profile cannot join the cluster, so an unauthorized system cannot obtain or use names inside the GNS subdomain.</p>
<div class="infobox-note">
<p class="notep1">Caution:</p>
Granting <code dir="ltr">root</code> access by domain can be used to obtain unauthorized access to systems. System administrators should see their operating system documentation for the risks associated with using <code dir="ltr">no_root_squash</code>.</div>
<p>After changing <code dir="ltr">/etc/exports</code>, reload the file system mount using the following command:</p>
<pre dir="ltr"># /usr/sbin/exportfs -avr
</pre></div>
<!-- class="sect2" -->
<a id="BABHHDDI"></a>
<div id="CWAIX569" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.3.2</span> Checking Operating System NFS Mount and Buffer Size Parameters</h3>
<p>On Oracle Grid Infrastructure cluster member nodes, you must set the values for the NFS buffer size parameters <code dir="ltr"><a id="sthref579"></a><a id="sthref580"></a><a id="sthref581"></a><a id="sthref582"></a>rsize</code> and <code dir="ltr">wsize</code> to 32768.</p>
<p>The NFS client-side mount options for binaries are:</p>
<pre dir="ltr">rw,bg,hard,nointr,tcp,nfsvers=3,timeo=600,rsize=32768,wsize=32768,actimeo=0
</pre>
<p>If you have Oracle Grid Infrastructure binaries on an NFS mount, then you must not include the <code dir="ltr">nosuid</code> option.</p>
<p>The NFS client-side mount options for Oracle Clusterware files (OCR and voting disk files) are:</p>
<pre dir="ltr">rw,bg,hard,nointr,rsize=32768,wsize=32768,tcp,vers=3,timeo=600,actimeo=0
</pre>
<p>Update the <code dir="ltr">/etc/fstab</code> file on each node with an entry containing the NFS mount options for your platform. For example, if your platform is x86-64, and you are creating a mount point for Oracle Clusterware files, then update the <code dir="ltr">/etc/fstab</code> files with an entry similar to the following:</p>
<pre dir="ltr">nfs_server:/vol/grid  /u02/oracle/cwfiles nfs \
rw,bg,hard,nointr,rsize=32768,wsize=32768,tcp,vers=3,timeo=600,actimeo=0
</pre>
<p>Note that mount point options are different for Oracle software binaries, Oracle Clusterware files (OCR and voting disks), and data files.</p>
<p>To create a mount point for binaries only, provide an entry similar to the following for a binaries mount point:</p>
<pre dir="ltr"><span class="italic">nfs_server</span>:/vol/bin /u02/oracle/grid nfs \
rw,bg,hard,nointr,rsize=32768,wsize=32768,tcp,vers=3,timeo=600,actimeo=0,suid 0 0
</pre>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
My Oracle Support bulletin 359515.1, &#34;Mount Options for Oracle Files When Used with NAS Devices&#34; for the most current information about mount options, available from the following URL:
<p><code dir="ltr"><a href="https://support.oracle.com/CSP/main/article?cmd=show&amp;type=NOT&amp;id=359515.1">https://support.oracle.com/CSP/main/article?cmd=show&amp;type=NOT&amp;id=359515.1</a></code></p>
</div>
<div class="infobox-note">
<p class="notep1">Note:</p>
Refer to your storage vendor documentation for additional information about mount options.</div>
</div>
<!-- class="sect2" -->
<a id="BABJADBH"></a>
<div id="CWAIX570" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.3.3</span> Checking <a id="sthref583"></a>NFS Mount and Buffer Size Parameters for Oracle RAC</h3>
<p>If you use NFS mounts for Oracle RAC files, then you must mount NFS volumes used for storing database files with special mount options on each node that has an Oracle RAC instance. When mounting an NFS file system, Oracle recommends that you use the same mount point options that your NAS vendor used when certifying the device. Refer to your device documentation or contact your vendor for information about recommended mount-point options.</p>
<p>Update the <code dir="ltr">/etc/fstab</code> file on each node with an entry similar to the following:</p>
<pre dir="ltr"><span class="italic">nfs_server</span>:/vol/DATA/oradata  /u02/oradata     nfs\   
rw,bg,hard,nointr,tcp,nfsvers=3,timeo=600,rsize=32768,wsize=32768,actimeo=0
</pre>
<p>The mandatory mount options comprise the minimum set of mount options that you must use while mounting the NFS volumes. These mount options are essential to protect the integrity of the data and to prevent any database corruption. Failure to use these mount options may result in the generation of file access errors. see your operating system or NAS device documentation for more information about the specific options supported on your platform.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
My Oracle Support Note 359515.1 for updated NAS mount option information, available at the following URL:
<pre dir="ltr"><a href="https://support.oracle.com/CSP/main/article?cmd=show&amp;type=NOT&amp;id=359515.1">https://support.oracle.com/CSP/main/article?cmd=show&amp;type=NOT&amp;id=359515.1</a>
</pre></div>
</div>
<!-- class="sect2" -->
<a id="BABFIEDI"></a>
<div id="CWAIX571" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.3.4</span> Setting TCP Network Protocol Buffer for Direct NFS Client</h3>
<p>By default, the network buffer size is set to 1 MB for TCP, and 2 MB for UDP. The TCP buffer size can set a limit on file transfers, which can negatively affect performance for Direct NFS Client users.</p>
<p>To check the current TCP buffer size, enter the following command:</p>
<pre dir="ltr"># /usr/sbin/no -o sb_max
</pre>
<p>Oracle recommends that you set the value based on the link speed of your servers. For example:</p>
<pre dir="ltr"># /usr/sbin/no -p -o sb_max=524288
</pre>
<p>To make TCP changes permanent, add the following lines into the <code dir="ltr">/etc/rc.net</code> file:</p>
<pre dir="ltr">/usr/sbin/no -o sb_max=524288
</pre></div>
<!-- class="sect2" -->
<a id="CHDIJGCA"></a>
<div id="CWAIX431" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.3.5</span> Enabling <a id="sthref584"></a><a id="sthref585"></a>Direct NFS Client Oracle Disk Manager Control of NFS</h3>
<p>Complete the following procedure to enable Direct NFS Client:</p>
<ol>
<li>
<p>Create an <code dir="ltr">oranfstab</code> file with the following attributes for each NFS server you configure for access using Direct NFS Client:</p>
<ul>
<li>
<p><span class="bold">server</span>: The NFS server name.</p>
</li>
<li>
<p><span class="bold">local</span>: Up to four paths on the database host, specified by IP address or by name, as displayed using the <code dir="ltr">ifconfig</code> command run on the database host.</p>
</li>
<li>
<p><span class="bold">path</span>: Up to four network paths to the NFS server, specified either by IP address, or by name, as displayed using the <code dir="ltr">ifconfig</code> command on the NFS server.</p>
</li>
<li>
<p><span class="bold">export</span>: The exported path from the NFS server.</p>
</li>
<li>
<p><span class="bold">mount</span>: The corresponding local mount point for the exported volume.</p>
</li>
<li>
<p><span class="bold">mnt_timeout</span>: Specifies (in seconds) the time Direct NFS Client should wait for a successful mount before timing out. This parameter is optional. The default timeout is 10 minutes (<code dir="ltr">600</code>).</p>
</li>
<li>
<p><span class="bold">nfs_version</span>: Specifies the NFS protocol version Direct NFS Client uses. Possible values are NFSv3, NFSv4 and NFSv4.1. The default version is NFSv3. If you select NFSv4.x, then you must configure the value in <code dir="ltr">oranfstab</code> for <code dir="ltr">nfs_version</code>.</p>
</li>
<li>
<p><span class="bold">dontroute</span>: Specifies that outgoing messages should not be routed by the operating system, but instead sent using the IP address to which they are bound.</p>
</li>
<li>
<p><span class="bold">management:</span> Enables Direct NFS Client to use the management interface for SNMP queries. You can use this parameter if SNMP is running on separate management interfaces on the NFS server. The default value is the server parameter value.</p>
</li>
<li>
<p><span class="bold">community:</span> Specifies the community string for use in SNMP queries. Default value is public.</p>
</li>
</ul>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a class="olink TGDBA95342" href="../TGDBA/pfgrf_os.htm#TGDBA95342"><span class="italic">Oracle Database Performance Tuning Guide</span></a> for more information about limiting asynchronous I/O</div>
<p>The examples that follow show three possible NFS server entries in <code dir="ltr">oranfstab</code>. A single <code dir="ltr">oranfstab</code> can have multiple NFS server entries.</p>
<div id="CWAIX572" class="example">
<p class="titleinexample"><a id="sthref586"></a>Example 6-1 Using Local and Path NFS Server Entries</p>
<p>The following example uses both local and path. Because local and path are in different subnets, there is no need to specify <code dir="ltr">dontroute</code>.</p>
<pre dir="ltr">server: MyDataServer1
local: 192.0.2.0
path: 192.0.2.1
local: 192.0.100.0
path: 192.0.100.1
export: /vol/oradata1 mount: /mnt/oradata1
nfs_version: nfsv3
community: private
</pre></div>
<!-- class="example" -->
<div id="CWAIX573" class="example">
<p class="titleinexample"><a id="sthref587"></a>Example 6-2 Using Local and Path in the Same Subnet, with dontroute</p>
<p>The following example shows local and path in the same subnet. <code dir="ltr">dontroute</code> is specified in this case:</p>
<pre dir="ltr">server: MyDataServer2
local: 192.0.2.0
path: 192.0.2.128
local: 192.0.2.1
path: 192.0.2.129
dontroute
export: /vol/oradata2 mount: /mnt/oradata2
nfs_version: nfsv4
management: 192.0.10.128
</pre></div>
<!-- class="example" -->
<div id="CWAIX574" class="example">
<p class="titleinexample"><a id="sthref588"></a>Example 6-3 Using Names in Place of IP Addresses, with Multiple Exports</p>
<pre dir="ltr">server: MyDataServer3
local: LocalPath1
path: NfsPath1
local: LocalPath2
path: NfsPath2
local: LocalPath3
path: NfsPath3
local: LocalPath4
path: NfsPath4
dontroute
export: /vol/oradata3 mount: /mnt/oradata3
export: /vol/oradata4 mount: /mnt/oradata4
export: /vol/oradata5 mount: /mnt/oradata5
export: /vol/oradata6 mount: /mnt/oradata6
</pre></div>
<!-- class="example" --></li>
<li>
<p>By default, Direct NFS Client is installed in an enabled state. However, if Direct NFS Client is disabled and you want to enable it, complete the following steps on each node. If you use a shared Grid home for the cluster, then complete the following steps in the shared Grid home:</p>
<ol>
<li>
<p>Log in as the Oracle Grid Infrastructure installation owner.</p>
</li>
<li>
<p>Change directory to <code dir="ltr"><span class="codeinlineitalic">Grid_home</span></code><code dir="ltr">/rdbms/lib</code>.</p>
</li>
<li>
<p>Enter the following commands:</p>
<pre dir="ltr">$ make -f ins_rdbms.mk dnfs_on
</pre></li>
</ol>
</li>
</ol>
</div>
<!-- class="sect2" -->
<a id="CHDDEHIB"></a>
<div id="CWAIX430" class="sect2">
<h3 class="sect2"><span class="secnum">6.3.6</span> Specifying Network Paths with the Oranfstab File</h3>
<p>Direct NFS Client can use up to four network paths defined in the <code dir="ltr">oranfstab</code> file for an NFS server. The Direct NFS Client performs load balancing across all specified paths. If a specified path fails, then Direct NFS Client reissues I/O commands over any remaining paths.</p>
<p>Use the following SQL*Plus views for managing Direct NFS Client in a cluster environment:</p>
<ul>
<li>
<p><span class="bold">gv$dnfs_servers</span>: Shows a table of servers accessed using Direct NFS Client.</p>
</li>
<li>
<p><span class="bold">gv$dnfs_files</span>: Shows a table of files currently open using Direct NFS Client.</p>
</li>
<li>
<p><span class="bold">gv$dnfs_channels</span>: Shows a table of open network paths (or channels) to servers for which Direct NFS Client is providing files.</p>
</li>
<li>
<p><span class="bold">gv$dnfs_stats</span>: Shows a table of performance statistics for Direct NFS Client.</p>
</li>
</ul>
<div class="infobox-note">
<p class="notep1">Note:</p>
Use <code dir="ltr">v$</code> views for single instances, and <code dir="ltr">gv$</code> views for Oracle Clusterware and Oracle RAC storage.</div>
</div>
<!-- class="sect2" -->
<a id="BABJBEEG"></a>
<div class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.3.7</span> Enabling Hybrid Columnar Compression on Direct NFS Client</h3>
<p>To enable Hybrid Columnar Compression (HCC) on Direct NFS Client, perform the following steps:</p>
<ol>
<li>
<p>Ensure that SNMP is enabled on the ZFS Storage Server. For example:</p>
<pre dir="ltr">$ snmpinfo -v1 -c public <span class="codeinlineitalic">server_name</span> .1.3.6.1.4.1.42.2.225.1.4.2.0
enterprises.42.2.225.1.4.2.0 = 53:75:6e:20:53:74:6f:72:61:67:65:20:37:34:31:30
</pre>
<p>The above sequence of bytes 53:75:6e:....:31:30 is an ascii character representation for Sun Storage 7410.</p>
</li>
<li>
<p>If SNMP is enabled on an interface other than the NFS server, then configure <code dir="ltr">oranfstab</code> using the <code dir="ltr">management</code> parameter.</p>
</li>
<li>
<p>If SNMP is configured using a community string other than public, then configure <code dir="ltr">oranfstab</code> file using the community parameter.</p>
</li>
<li>
<p>Ensure that <code dir="ltr">libnetsnmp.so</code> is installed. If <code dir="ltr">libnetsnmp.so</code> is not installed, download and install it from the following URL:</p>
<p><code dir="ltr"><a href="https://oss.oracle.com/netsnmp/">https://oss.oracle.com/netsnmp/</a></code></p>
<div class="infobox-note">
<p class="notep1">Note:</p>
On IBM AIX on POWER Systems (64-Bit), HCC is not supported for block-based devices.</div>
</li>
</ol>
</div>
<!-- class="sect2" -->
<a id="CDEIIIAI"></a>
<div id="CWAIX267" class="sect2">
<h3 class="sect2"><span class="secnum">6.3.8</span> Creating Directories for Oracle Clusterware Files on Shared File Systems<a id="sthref589"></a><a id="sthref590"></a><a id="sthref591"></a></h3>
<p>Use the following instructions to create directories for Oracle Clusterware files. You can also configure shared file systems for the Oracle Database and recovery files.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
For NFS or GPFS storage, you must complete this procedure only if you want to place the Oracle Clusterware files on a separate file system to the Oracle base directory.</div>
<p>To create directories for the Oracle Clusterware files on separate file systems from the Oracle base directory, follow these steps:</p>
<ol>
<li>
<p>If necessary, configure the shared file systems to use and mount them on each node.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
The mount point that you use for the file system must be identical on each node. Ensure that the file systems are configured to mount automatically when a node restarts.</div>
</li>
<li>
<p>Use the <code dir="ltr">df</code> command to determine the free disk space on each mounted file system.</p>
</li>
<li>
<p>From the display, identify the file systems to use. Choose a file system with a minimum of 600 MB of free disk space (one OCR and one voting disk, with external redundancy).</p>
<p>If you are using the same file system for multiple file types, then add the disk space requirements for each type to determine the total disk space requirement.</p>
</li>
<li>
<p>Note the names of the mount point directories for the file systems that you identified.</p>
</li>
<li>
<p>If the user performing installation (typically, <code dir="ltr">grid</code> or <code dir="ltr">oracle</code>) has permissions to create directories on the storage location where you plan to install Oracle Clusterware files, then OUI creates the Oracle Clusterware file directory.</p>
<p>If the user performing installation does not have write access, then you must create these directories manually using commands similar to the following to create the recommended subdirectories in each of the mount point directories and set the appropriate owner, group, and permissions on the directory. For example, where the user is <code dir="ltr">oracle</code>, and the Oracle Clusterware file storage area is <code dir="ltr">cluster</code>:</p>
<pre dir="ltr"># mkdir /<span class="italic">mount_point</span>/cluster
# chown oracle:oinstall /<span class="italic">mount_point</span>/cluster
# chmod 775 /<span class="italic">mount_point</span>/cluster
</pre>
<div class="infobox-note">
<p class="notep1">Note:</p>
After installation, directories in the installation path for the OCR files should be owned by <code dir="ltr">root</code>, and not writable by any account other than <code dir="ltr">root</code>.</div>
</li>
</ol>
<p>When you have completed creating a subdirectory in the mount point directory, and set the appropriate owner, group, and permissions, you have completed OCFS2 or NFS configuration for Oracle Grid Infrastructure.</p>
</div>
<!-- class="sect2" -->
<a id="CDEIGEAH"></a>
<div id="CWAIX268" class="sect2">
<h3 class="sect2"><span class="secnum">6.3.9</span> Creating Directories for Oracle Database Files on Shared File Systems<a id="sthref592"></a><a id="sthref593"></a><a id="sthref594"></a></h3>
<p>Use the following instructions to create directories for shared file systems for Oracle Database and recovery files (for example, for an Oracle RAC database).</p>
<ol>
<li>
<p>If necessary, configure the shared file systems and mount them on each node.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
The mount point that you use for the file system must be identical on each node. Ensure that the file systems are configured to mount automatically when a node restarts.</div>
</li>
<li>
<p>Use the <code dir="ltr">df -h</code> command to determine the free disk space on each mounted file system.</p>
</li>
<li>
<p>From the display, identify the file systems:</p>
<div class="inftblhruleinformal">
<table class="cellalignment4202" title="File System Requirements for Datafiles and Recovery Files" summary="This table describes the file system requirements for Oracle database and recovery file storage" dir="ltr">
<thead>
<tr class="cellalignment4191">
<th class="cellalignment4203" id="r1c1-t38">File Type</th>
<th class="cellalignment4203" id="r1c2-t38">File System Requirements</th>
</tr>
</thead>
<tbody>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r2c1-t38" headers="r1c1-t38">Database files</td>
<td class="cellalignment4197" headers="r2c1-t38 r1c2-t38">Choose either:
<ul>
<li>
<p>A single file system with at least 1.5 GB of free disk space.</p>
</li>
<li>
<p>Two or more file systems with at least 1.5 GB of free disk space in total.</p>
</li>
</ul>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r3c1-t38" headers="r1c1-t38">Recovery files</td>
<td class="cellalignment4197" headers="r3c1-t38 r1c2-t38">Choose a file system with at least 2 GB of free disk space.</td>
</tr>
</tbody>
</table>
<br/></div>
<!-- class="inftblhruleinformal" -->
<p>If you are using the same file system for multiple file types, then add the disk space requirements for each type to determine the total disk space requirement.</p>
</li>
<li>
<p>Note the names of the mount point directories for the file systems that you identified.</p>
</li>
<li>
<p>If the user performing installation (typically, <code dir="ltr">oracle</code>) has permissions to create directories on the disks where you plan to install Oracle Database, then DBCA creates the Oracle Database file directory, and the Recovery file directory.</p>
<p>If the user performing installation does not have write access, then you must create these directories manually using commands similar to the following to create the recommended subdirectories in each of the mount point directories and set the appropriate owner, group, and permissions on them:<a id="sthref595"></a><a id="sthref596"></a><a id="sthref597"></a><a id="sthref598"></a><a id="sthref599"></a><a id="sthref600"></a><a id="sthref601"></a><a id="sthref602"></a><a id="sthref603"></a></p>
<ul>
<li>
<p>Database file directory:</p>
<pre dir="ltr"># mkdir /<span class="italic">mount_point</span>/oradata
# chown oracle:oinstall /<span class="italic">mount_point</span>/oradata
# chmod 775 /<span class="italic">mount_point</span>/oradata
</pre></li>
</ul>
<ul>
<li>
<p>Recovery file directory (Fast Recovery Area):</p>
<pre dir="ltr"># mkdir /<span class="italic">mount_point</span>/fast_recovery_area
# chown oracle:oinstall /<span class="italic">mount_point</span>/fast_recovery_area
# chmod 775 /<span class="italic">mount_point</span>/fast_recovery_area
</pre></li>
</ul>
</li>
</ol>
<p>By making members of the <code dir="ltr">oinstall</code> group owners of these directories, this permits them to be read by <a id="sthref604"></a><a id="sthref605"></a>multiple Oracle homes, including those with different OSDBA groups.</p>
<p>When you have completed creating subdirectories in each of the mount point directories, and set the appropriate owner, group, and permissions, you have completed NFS configuration for Oracle Database shared storage.</p>
</div>
<!-- class="sect2" -->
<a id="CHDJBDGA"></a>
<div id="CWAIX435" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.3.10</span> Disabling <a id="sthref606"></a>Direct NFS Client Oracle Disk Management Control of NFS</h3>
<p><a id="sthref607"></a>Complete the following steps to disable the Direct NFS Client:</p>
<ol>
<li>
<p>Log in as the Oracle Grid Infrastructure installation owner, and disable the Direct NFS Client using the following commands, where <code dir="ltr"><span class="codeinlineitalic">Grid_home</span></code> is the path to the Oracle Grid Infrastructure home:</p>
<pre dir="ltr">$ cd <span class="italic">Grid_home</span>/rdbms/lib
$ make -f ins_rdbms.mk dnfs_off
</pre>
<p>Enter these commands on each node in the cluster, or on the shared Grid home if you are using a shared home for the Oracle Grid Infrastructure installation.</p>
</li>
<li>
<p>Remove the <code dir="ltr">oranfstab</code> file.</p>
</li>
</ol>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you remove an NFS path that Oracle Database is using, then you must restart the database for the change to be effective.</div>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" -->
<a id="CDEDIGAH"></a>
<div id="CWAIX269" class="sect1">
<h2 class="sect1"><span class="secnum">6.4</span> Oracle Automatic Storage Management Storage Configuration</h2>
<p>Review the following sections to configure storage for Oracle Automatic Storage Management:</p>
<ul>
<li>
<p><a href="#CDEBDDHD">Configuring Storage for Oracle Automatic Storage Management</a></p>
</li>
<li>
<p><a href="#CDEFICBE">Using an Existing Oracle ASM Disk Group</a></p>
</li>
<li>
<p><a href="#CDEBFGAD">Configuring Disk Devices for Oracle ASM</a></p>
</li>
<li>
<p><a href="#CDEHFDGG">Using Disk Groups with Oracle Database Files on Oracle ASM</a></p>
</li>
<li>
<p><a href="#BABFJECI">Configuring Oracle Automatic Storage Management Cluster File System</a></p>
</li>
<li>
<p><a href="#CDEEAIHI">Upgrading Existing Oracle ASM Instances</a></p>
</li>
</ul>
<a id="CDEBDDHD"></a>
<div id="CWAIX270" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.4.1</span> Configuring Storage for Oracle Automatic Storage Management</h3>
<p>This section describes how to configure storage for use with Oracle Automatic Storage Management (Oracle ASM).</p>
<ul>
<li>
<p><a href="#CDECEBJE">Identifying Storage Requirements for Oracle Automatic Storage Management</a></p>
</li>
<li>
<p><a href="#CDEIBCBF">Creating Files on a NAS Device for Use with Oracle ASM</a></p>
</li>
<li>
<p><a href="#CDEFICBE">Using an Existing Oracle ASM Disk Group</a></p>
</li>
<li>
<p><a href="#CDEBFGAD">Configuring Disk Devices for Oracle ASM</a></p>
</li>
</ul>
<a id="CDECEBJE"></a>
<div id="CWAIX271" class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3"><span class="secnum">6.4.1.1</span> Identifying Storage Requirements for Oracle Automatic Storage Management</h4>
<p>To identify the storage requirements for using Oracle ASM, you must determine how many devices and the amount of free disk space that you require. To complete this task, follow these steps:</p>
<ol>
<li>
<p>Determine whether you want to use Oracle ASM for Oracle Clusterware files (OCR and voting files), Oracle Database files, recovery files, or all files except for Oracle Clusterware or Oracle Database binaries. Oracle Database files include data files, control files, redo log files, the server parameter file, and the password file.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
<ul>
<li>You do not have to use the same storage mechanism for Oracle Clusterware, Oracle Database files and recovery files. You can use a shared file system for one file type and Oracle ASM for the other.</li>
<li>
<p>There are two types of Oracle Clusterware files: OCR files and voting files. Each type of file can be stored on either Oracle ASM or a cluster file system. All the OCR files or all the voting files must use the same type of storage. You cannot have some OCR files stored in Oracle ASM and other OCR files in a cluster file system. However, you can use one type of storage for the OCR files and a different type of storage for the voting files if all files of each type use the same type of storage.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Choose the Oracle ASM redundancy level to use for the Oracle ASM disk group.<a id="sthref608"></a><a id="sthref609"></a><a id="sthref610"></a><a id="sthref611"></a><a id="sthref612"></a><a id="sthref613"></a><a id="sthref614"></a><a id="sthref615"></a><a id="sthref616"></a></p>
<p>The redundancy level that you choose for the Oracle ASM disk group determines how Oracle ASM mirrors files in the disk group and determines the number of disks and amount of free disk space that you require. If the voting files are in a disk group, then the disk groups that contain Oracle Clusterware files (OCR and voting files) have a higher minimum number of failure groups than other disk groups because the voting files are stored in quorum failure groups.</p>
<p>A quorum failure group is a special type of failure group that is used to store the Oracle Clusterware voting files. The quorum failure group is used to ensure that a quorum of the specified failure groups are available. When Oracle ASM mounts a disk group that contains Oracle Clusterware files, the quorum failure group is used to determine if the disk group can be mounted in the event of the loss of one or more failure groups. Disks in the quorum failure group do not contain user data, therefore a quorum failure group is not considered when determining redundancy requirements in respect to storing user data.</p>
<p>The redundancy levels are as follows:</p>
<ul>
<li>
<p>External redundancy</p>
<p>An external redundancy disk group requires a minimum of one disk device. The effective disk space in an external redundancy disk group is the sum of the disk space in all of its devices.</p>
<p><a id="sthref617"></a>Because Oracle ASM does not mirror data in an external redundancy disk group, Oracle recommends that you use external redundancy with storage devices such as RAID, or other similar devices that provide their own data protection mechanisms.</p>
</li>
<li>
<p>Normal redundancy</p>
<p>In a normal redundancy disk group, to increase performance and reliability, Oracle ASM by default uses two-way mirroring. A normal redundancy disk group requires a minimum of two disk devices (or two failure groups). The effective disk space in a normal redundancy disk group is half the sum of the disk space in all of its devices.</p>
<p>For Oracle Clusterware files, a normal redundancy disk group requires a minimum of three disk devices (two of the three disks are used by failure groups and all three disks are used by the quorum failure group) and provides three voting files and one OCR (one primary and one secondary copy). With normal redundancy, the cluster can survive the loss of one failure group.</p>
<p>For most installations, Oracle recommends that you select normal redundancy.</p>
</li>
<li>
<p>High redundancy</p>
<p>In a high redundancy disk group, Oracle ASM uses three-way mirroring to increase performance and provide the highest level of reliability. A high redundancy disk group requires a minimum of three disk devices (or three failure groups). The effective disk space in a high redundancy disk group is one-third the sum of the disk space in all of its devices.</p>
<p>For Oracle Clusterware files, a high redundancy disk group requires a minimum of five disk devices (three of the five disks are used by failure groups and all five disks are used by the quorum failure group) and provides five voting files and one OCR (one primary and two secondary copies). With high redundancy, the cluster can survive the loss of two failure groups.</p>
<p>While high redundancy disk groups do provide a high level of data protection, you should consider the greater cost of additional storage devices before deciding to select high redundancy disk groups.</p>
</li>
</ul>
<div class="infobox-note">
<p class="notep1">Note:</p>
After a disk group is created, you cannot alter the redundancy level of the disk group.</div>
</li>
<li>
<p>Determine the total amount of disk space that you require for Oracle Clusterware files, and for the database files and recovery files.<a id="sthref618"></a><a id="sthref619"></a><a id="sthref620"></a><a id="sthref621"></a><a id="sthref622"></a><a id="sthref623"></a></p>
<p><a id="sthref624"></a><a id="sthref625"></a>Use <a href="#BABGGGAH">Table 6-4</a> and <a href="#BABDEDHA">Table 6-5</a> to determine the minimum number of disks and the minimum disk space requirements for installing Oracle Clusterware files, and installing the starter database, where you have voting files in a separate disk group:<a id="sthref626"></a><a id="sthref627"></a><a id="sthref628"></a><a id="sthref629"></a></p>
<div id="CWAIX10029" class="tblformal">
<p class="titleintable"><a id="sthref630"></a><a id="BABGGGAH"></a>Table 6-4 Total Oracle Clusterware Storage Space Required by Redundancy Type</p>
<table class="cellalignment4204" title="Total Oracle Clusterware Storage Space Required by Redundancy Type" summary="disk space requirements for external, normal and high redundancy" dir="ltr">
<thead>
<tr class="cellalignment4191">
<th class="cellalignment4203" id="r1c1-t42">Redundancy Level</th>
<th class="cellalignment4203" id="r1c2-t42">Minimum Number of Disks</th>
<th class="cellalignment4203" id="r1c3-t42">Oracle Cluster Registry (OCR) Files</th>
<th class="cellalignment4203" id="r1c4-t42">Voting Files</th>
<th class="cellalignment4203" id="r1c5-t42">Both File Types</th>
<th class="cellalignment4203" id="r1c6-t42">Total Storage with Grid Infrastructure Management Repository</th>
</tr>
</thead>
<tbody>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r2c1-t42" headers="r1c1-t42">
<p>External</p>
</td>
<td class="cellalignment4197" headers="r2c1-t42 r1c2-t42">
<p>1</p>
</td>
<td class="cellalignment4197" headers="r2c1-t42 r1c3-t42">
<p>400 MB</p>
</td>
<td class="cellalignment4197" headers="r2c1-t42 r1c4-t42">
<p>300 MB</p>
</td>
<td class="cellalignment4197" headers="r2c1-t42 r1c5-t42">
<p>700 MB</p>
</td>
<td class="cellalignment4197" headers="r2c1-t42 r1c6-t42">
<p>At least 5.9 GB for a cluster with 4 nodes or less (5.2 GB + 400 MB + 300 MB).</p>
<p>Additional space required for clusters with 5 or more nodes. For example, a six-node cluster allocation should be at least 6.9 GB:</p>
<p>(5.2 GB +2*(500 MB) +400 MB + 300 MB).</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r3c1-t42" headers="r1c1-t42">
<p>Normal</p>
</td>
<td class="cellalignment4197" headers="r3c1-t42 r1c2-t42">
<p>3</p>
</td>
<td class="cellalignment4197" headers="r3c1-t42 r1c3-t42">
<p>At least 400 MB for each failure group, or 800 MB</p>
</td>
<td class="cellalignment4197" headers="r3c1-t42 r1c4-t42">
<p>900 MB</p>
</td>
<td class="cellalignment4197" headers="r3c1-t42 r1c5-t42">
<p>1.7 GB<a id="sthref631" href="#sthref631" onclick="footdisplay(1,&#34;If you create a disk group during installation, then it must be at least 2 GB.&#34;)"><sup class="tablefootnote">Foot&nbsp;1&nbsp;</sup></a></p>
</td>
<td class="cellalignment4197" headers="r3c1-t42 r1c6-t42">
<p>At least 12.1 GB for a cluster with 4 nodes or less (2*5.2 GB + 2*400 MB + 3*300 MB).</p>
<p>Additional space required for clusters with 5 or more nodes. For example, for a six-node cluster allocation should be at least 14.1 GB:</p>
<p>(2 * (5.2 GB +2*(500 MB)) +(2 * 400 MB) +(3 * 300 MB)).</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r4c1-t42" headers="r1c1-t42">
<p>High</p>
</td>
<td class="cellalignment4197" headers="r4c1-t42 r1c2-t42">
<p>5</p>
</td>
<td class="cellalignment4197" headers="r4c1-t42 r1c3-t42">
<p>At least 400 MB for each failure group, or 1.2 GB</p>
</td>
<td class="cellalignment4197" headers="r4c1-t42 r1c4-t42">
<p>1.5 GB</p>
</td>
<td class="cellalignment4197" headers="r4c1-t42 r1c5-t42">
<p>2.7 GB</p>
</td>
<td class="cellalignment4197" headers="r4c1-t42 r1c6-t42">
<p>At least 18.3 GB for a cluster with 4 nodes or less (3* 5.2 GB + 3*400 MB + 5*300 MB).</p>
<p>Additional space required for clusters with 5 or more nodes. For example, for a six-node cluster allocation should be at least 21.3 GB:</p>
<p>(3* (5.2 GB +2*(500 MB))+(3 * 400 MB) +(5 * 300 MB)).</p>
</td>
</tr>
</tbody>
</table>
<br/></div>
<!-- class="tblformal" -->
<p class="tablefootnote"><sup class="tablefootnote">Footnote&nbsp;1&nbsp;</sup>If you create a disk group during installation, then it must be at least 2 GB.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
If the voting files are in a disk group, be aware that disk groups with Oracle Clusterware files (OCR and voting files) have a higher minimum number of failure groups than other disk groups.
<p>If you create a disk group as part of the installation in order to install the OCR and voting files, then the installer requires that you create these files on a disk group with at least 2 GB of available space.</p>
</div>
<div id="CWAIX10030" class="tblformal">
<p class="titleintable"><a id="sthref632"></a><a id="BABDEDHA"></a>Table 6-5 Total Oracle Database Storage Space Required by Redundancy Type</p>
<table class="cellalignment4204" title="Total Oracle Database Storage Space Required by Redundancy Type" summary="database disk space requirements" dir="ltr">
<thead>
<tr class="cellalignment4191">
<th class="cellalignment4203" id="r1c1-t44">Redundancy Level</th>
<th class="cellalignment4203" id="r1c2-t44">Minimum Number of Disks</th>
<th class="cellalignment4203" id="r1c3-t44">Database Files</th>
<th class="cellalignment4203" id="r1c4-t44">Recovery Files</th>
<th class="cellalignment4203" id="r1c5-t44">Both File Types</th>
</tr>
</thead>
<tbody>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r2c1-t44" headers="r1c1-t44">
<p>External</p>
</td>
<td class="cellalignment4197" headers="r2c1-t44 r1c2-t44">
<p>1</p>
</td>
<td class="cellalignment4197" headers="r2c1-t44 r1c3-t44">
<p>1.5 GB</p>
</td>
<td class="cellalignment4197" headers="r2c1-t44 r1c4-t44">
<p>3 GB</p>
</td>
<td class="cellalignment4197" headers="r2c1-t44 r1c5-t44">
<p>4.5 GB</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r3c1-t44" headers="r1c1-t44">
<p>Normal</p>
</td>
<td class="cellalignment4197" headers="r3c1-t44 r1c2-t44">
<p>2</p>
</td>
<td class="cellalignment4197" headers="r3c1-t44 r1c3-t44">
<p>3 GB</p>
</td>
<td class="cellalignment4197" headers="r3c1-t44 r1c4-t44">
<p>6 GB</p>
</td>
<td class="cellalignment4197" headers="r3c1-t44 r1c5-t44">
<p>9 GB</p>
</td>
</tr>
<tr class="cellalignment4191">
<td class="cellalignment4197" id="r4c1-t44" headers="r1c1-t44">
<p>High</p>
</td>
<td class="cellalignment4197" headers="r4c1-t44 r1c2-t44">
<p>3</p>
</td>
<td class="cellalignment4197" headers="r4c1-t44 r1c3-t44">
<p>4.5 GB</p>
</td>
<td class="cellalignment4197" headers="r4c1-t44 r1c4-t44">
<p>9 GB</p>
</td>
<td class="cellalignment4197" headers="r4c1-t44 r1c5-t44">
<p>13.5 GB</p>
</td>
</tr>
</tbody>
</table>
<br/></div>
<!-- class="tblformal" --></li>
<li>
<p>Determine an allocation unit size. Every Oracle ASM disk is divided into allocation units (AU). An allocation unit is the fundamental unit of allocation within a disk group. You can select the AU Size value from 1, 2, 4, 8, 16, 32 or 64 MB, depending on the specific disk group compatibility level. The default value is set to 1 MB.</p>
</li>
<li>
<p>For Oracle Clusterware installations, you must also add additional disk space for the Oracle ASM metadata. You can use the following formula to calculate the disk space requirements (in MB) for OCR and voting files, and the Oracle ASM metadata:</p>
<p>total = [2 * ausize * disks] + [redundancy * (ausize * (nodes * (clients + 1) + 30) + (64 * nodes) + 533)]</p>
<p>Where:</p>
<ul>
<li>
<p>redundancy = Number of mirrors: external = 1, normal = 2, high = 3.</p>
</li>
<li>
<p>ausize = Metadata AU size in megabytes.</p>
</li>
<li>
<p>nodes = Number of nodes in cluster.</p>
</li>
<li>
<p>clients - Number of database instances for each node.</p>
</li>
<li>
<p>disks - Number of disks in disk group.</p>
</li>
</ul>
<p>For example, for a four-node Oracle RAC installation, using three disks in a normal redundancy disk group, you require an additional X MB of space:</p>
<p>[2 * 1 * 3] + [2 * (1 * (4 * (4 + 1)+ 30)+ (64 * 4)+ 533)] = 1684 MB</p>
<p>To ensure high availability of Oracle Clusterware files on Oracle ASM, you need to have at least 2 GB of disk space for Oracle Clusterware files in three separate failure groups, with at least three physical disks. Each disk must have at least 1 GB of capacity to ensure that there is sufficient space to create Oracle Clusterware files.</p>
</li>
<li>
<p>Optionally, identify failure groups for the Oracle ASM disk group devices.<a id="sthref633"></a><a id="sthref634"></a><a id="sthref635"></a></p>
<p><a id="sthref636"></a><a id="sthref637"></a><a id="sthref638"></a><a id="sthref639"></a><a id="sthref640"></a>If you intend to use a normal or high redundancy disk group, then you can further protect your database against hardware failure by associating a set of disk devices in a custom failure group. By default, each device comprises its own failure group. However, if two disk devices in a normal redundancy disk group are attached to the same SCSI controller, then the disk group becomes unavailable if the controller fails. The controller in this example is a single point of failure.</p>
<p>To protect against failures of this type, you could use two SCSI controllers, each with two disks, and define a failure group for the disks attached to each controller. This configuration would enable the disk group to tolerate the failure of one SCSI controller.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
Define custom failure groups after installation, using the GUI tool ASMCA, the command line tool <code dir="ltr">asmcmd</code>, or SQL commands.
<p>If you define custom failure groups, then for failure groups containing database files only, you must specify a minimum of two failure groups for normal redundancy disk groups and three failure groups for high redundancy disk groups.</p>
<p>For failure groups containing database files and clusterware files, including voting files, you must specify a minimum of three failure groups for normal redundancy disk groups, and five failure groups for high redundancy disk groups.</p>
<p>Disk groups containing voting files must have at least 3 failure groups for normal redundancy or at least 5 failure groups for high redundancy. Otherwise, the minimum is 2 and 3 respectively. The minimum number of failure groups applies whether or not they are custom failure groups.</p>
</div>
</li>
<li>
<p>If you are sure that a suitable disk group does not exist on the system, then install or identify appropriate disk devices to add to a new disk group. Use the following guidelines when identifying appropriate disk devices:</p>
<ul>
<li>
<p>All of the devices in an Oracle ASM disk group should be the same size and have the same performance characteristics.</p>
</li>
<li>
<p>Do not specify multiple partitions on a single physical disk as a disk group device. Each disk group device should be on a separate physical disk.</p>
</li>
<li>
<p>Although you can specify a logical volume as a device in an Oracle ASM disk group, Oracle does not recommend their use because it adds a layer of complexity that is unnecessary with Oracle ASM. In addition, Oracle RAC requires a cluster logical volume manager in case you decide to use a logical volume with Oracle ASM and Oracle RAC.</p>
<p>Oracle recommends that if you choose to use a logical volume manager, then use the logical volume manager to represent a single LUN without striping or mirroring, so that you can minimize the impact of the additional storage layer.</p>
</li>
</ul>
</li>
</ol>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a class="olink OSTMG036" href="../OSTMG/GUID-BC612D35-5399-4A35-843E-CF76E3D3CDB5.htm#OSTMG036"><span class="italic">Oracle Automatic Storage Management Administrator&#39;s Guide</span></a> for information about allocation units</div>
</div>
<!-- class="sect3" -->
<a id="CDEIBCBF"></a>
<div id="CWAIX274" class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3"><span class="secnum">6.4.1.2</span> Creating Files on a NAS Device for Use with Oracle ASM</h4>
<p>If you have a certified NAS storage device, then you can create zero-padded files in an NFS mounted directory and use those files as disk devices in an Oracle ASM disk group.</p>
<p>To create these files, follow these steps:</p>
<ol>
<li>
<p>If necessary, create an exported directory for the disk group files on the NAS device.</p>
<p>Refer to the NAS device documentation for more information about completing this step.</p>
</li>
<li>
<p>Switch user to <code dir="ltr">root</code>.</p>
</li>
<li>
<p>Create a mount point directory on the local system. For example:</p>
<pre dir="ltr"># mkdir -p /mnt/asm
</pre></li>
<li>
<p>To ensure that the NFS file system is mounted when the system restarts, add an entry for the file system in the mount file <code dir="ltr">/etc/vfstab</code>.</p>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
My Oracle Support note 359515.1 for updated NAS mount option information, available at the following URL:
<pre dir="ltr"><a href="https://support.oracle.com">https://support.oracle.com</a>
</pre></div>
<p>For more information about editing the mount file for the operating system, refer to the <code dir="ltr">man</code> pages. For more information about recommended mount options, refer to <a href="#BABCHGHA">Configuring Operating System NFS Mount and Buffer Size Parameters</a>.</p>
</li>
<li>
<p>Enter a command similar to the following to mount the NFS file system on the local system:</p>
<pre dir="ltr"># mount /mnt/asm
</pre></li>
<li>
<p>Choose a name for the disk group to create. For example: <code dir="ltr">sales1</code>.</p>
</li>
<li>
<p>Create a directory for the files on the NFS file system, using the disk group name as the directory name. For example:</p>
<pre dir="ltr"># mkdir /mnt/asm/nfsdg
</pre></li>
<li>
<p>Use commands similar to the following to create the required number of zero-padded files in this directory:</p>
<pre dir="ltr"># dd if=/dev/zero of=/mnt/asm/nfsdg/disk1 bs=1024k count=1000
</pre>
<p>This example creates 1 GB files on the NFS file system. You must create one, two, or three files respectively to create an external, normal, or high redundancy disk group.</p>
</li>
<li>
<p>Enter commands similar to the following to change the owner, group, and permissions on the directory and files that you created, where the installation owner is <code dir="ltr">grid</code>, and the OSASM group is <code dir="ltr">asmadmin</code>:</p>
<pre dir="ltr"># chown -R grid:asmadmin /mnt/asm
# chmod -R 660 /mnt/asm
</pre></li>
<li>
<p>If you plan to install Oracle RAC or a standalone Oracle Database, then during installation, edit the Oracle ASM disk discovery string to specify a regular expression that matches the file names you created. For example:</p>
<pre dir="ltr">/mnt/asm/sales1/
</pre>
<div class="infobox-note">
<p class="notep1">Note:</p>
During installation, disk paths mounted on Oracle ASM are listed as default database storage candidate disks.</div>
</li>
</ol>
</div>
<!-- class="sect3" -->
<a id="CDEFICBE"></a>
<div id="CWAIX275" class="sect3">
<h4 class="sect3"><span class="secnum">6.4.1.3</span> Using an Existing Oracle ASM Disk Group</h4>
<p>Select from the following choices to store either database or recovery files in an existing Oracle ASM disk group, depending on installation method:</p>
<ul>
<li>
<p>If you select an installation method that runs Database Configuration Assistant in interactive mode, then you can decide whether you want to create a disk group, or to use an existing one.</p>
<p>The same choice is available to you if you use Database Configuration Assistant after the installation to create a database.</p>
</li>
<li>
<p>If you select an installation method that runs Database Configuration Assistant in noninteractive mode, then you must choose an existing disk group for the new database; you cannot create a disk group. However, you can add disk devices to an existing disk group if it has insufficient free space for your requirements.</p>
</li>
</ul>
<div class="infobox-note">
<p class="notep1">Note:</p>
The Oracle ASM instance that manages the existing disk group can be running in a different Oracle home directory.</div>
<p>To determine if an existing Oracle ASM disk group exists, or to determine if there is sufficient disk space in a disk group, you can use Oracle Enterprise Manager Cloud Control or the Oracle ASM command line tool (<code dir="ltr">asmcmd</code>) as follows:</p>
<ol>
<li>
<p>Connect to the Oracle ASM instance and start the instance if necessary:</p>
<pre dir="ltr">$ $ORACLE_HOME/bin/asmcmd
ASMCMD&gt; startup
</pre></li>
<li>
<p>Enter one of the following commands to view the existing disk groups, their redundancy level, and the amount of free disk space in each one:</p>
<pre dir="ltr">ASMCMD&gt; lsdb
</pre>
<p>or:</p>
<pre dir="ltr">$ORACLE_HOME/bin/asmcmd -p lsdg
</pre></li>
<li>
<p>From the output, identify a disk group with the appropriate redundancy level and note the free space that it contains.</p>
</li>
<li>
<p>If necessary, install or identify the additional disk devices required to meet the storage requirements listed in the previous section.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you are adding devices to an existing disk group, then Oracle recommends that you use devices that have the same size and performance characteristics as the existing devices in that disk group.</div>
</li>
</ol>
</div>
<!-- class="sect3" -->
<a id="CDEBFGAD"></a>
<div id="CWAIX276" class="sect3">
<h4 class="sect3"><span class="secnum">6.4.1.4</span> Configuring Disk Devices for Oracle ASM</h4>
<p>You can configure raw disks for use as Oracle Automatic Storage Management (Oracle ASM) disk groups. To use Oracle ASM with raw disks, you must create sufficient partitions for your data files, and then bind the partitions to raw disks. Make a list of the raw disk names you create for the data files, and have the list available during database installation.</p>
<p>In the following procedure, you are directed to set physical volume IDs (PVIDs) for raw disks. Oracle recommends that you complete the entire procedure, even if you are certain that you do not have PVIDs configured on your system, to prevent the possibility of configuration issues.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you intend to use Hitachi HDLM (<code dir="ltr">dmlf</code> devices) for storage, then ASM instances do not automatically discover the physical disks, but instead discover only the logical volume manager (LVM) disks. This is because the physical disks can only be opened by programs running as root.
<p>Physical disk paths have path names similar to the following:</p>
<pre dir="ltr">/dev/rdlmfdrv8
/dev/rdlmfdrv9
</pre></div>
<p>Use the following procedure to configure disks:</p>
<ol>
<li>
<p>If necessary, install the disks that you intend to use for the disk group and restart the system.</p>
</li>
<li>
<p>Identify or create the disks that you want to include in the Oracle ASM disk group. As the <code dir="ltr">root</code> user, enter the following command on any node to identify the device names for the disk devices that you want to use:</p>
<pre dir="ltr"># /usr/sbin/lspv | grep -i none 
</pre>
<p>This command displays information similar to the following for each disk device that is not configured in a volume group:</p>
<pre dir="ltr">hdisk17         0009005fb9c23648                    None  
</pre>
<p>In this example, <code dir="ltr">hdisk17</code> is the device name of the disk and <code dir="ltr">0009005fb9c23648</code> is the physical volume ID (PVID).</p>
</li>
<li>
<p>If a disk device that you want to use does not have a PVID, then enter a command similar to the following to assign one to it, where <code dir="ltr"><span class="codeinlineitalic">n</span></code> is the number of the <code dir="ltr">hdisk</code>:</p>
<pre dir="ltr"># chdev -l hdisk<span class="italic">n</span> -a pv=yes
</pre>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you have an existing PVID, then <code dir="ltr">chdev</code> overwrites the existing PVID. Be aware that if you have applications depending on the previous PVID, then they will fail.</div>
</li>
<li>
<p>On each of the other nodes, enter a command similar to the following to identify the device name associated with each PVID on that node:</p>
<pre dir="ltr"># /usr/sbin/lspv | grep -i &#34;0009005fb9c23648&#34;
</pre>
<p>The output from this command should be similar to the following:</p>
<pre dir="ltr">hdisk18         0009005fb9c23648                    None
</pre>
<p>In this example, the device name associated with the disk device (<code dir="ltr">hdisk18</code>) is different on this node.</p>
</li>
<li>
<p>If the device names are the same on all nodes, then enter commands similar to the following on all nodes to change the owner, group, and permissions on the character raw device files for the disk devices where <code dir="ltr">grid</code> is the Oracle Grid Infrastructure installation owner, and <code dir="ltr">asmadmin</code> is the OSASM group:</p>
<pre dir="ltr"># chown grid:asmadmin /dev/rhdisk<span class="italic">n</span>
# chmod 660 /dev/rhdisk<span class="italic">n</span>
</pre></li>
<li>
<p>To enable simultaneous access to a disk device from multiple nodes, you must set the appropriate Object Data Manager (ODM) attribute, depending on the type of reserve attribute used by your disks. The following section describes how to perform this task using hdisk logical names. Refer to your operating system documentation to find logical device names.</p>
<p>To determine the reserve setting your disks use, enter the following command, where <span class="italic">n</span> is the hdisk device number:</p>
<pre dir="ltr"># lsattr -E -l hdisk<span class="italic">n</span> | grep reserve_
</pre>
<p>The response is either a <code dir="ltr">reserve_lock</code> setting, or a <code dir="ltr">reserve_policy</code> setting. If the attribute is <code dir="ltr">reserve_lock</code>, then ensure that the setting is <code dir="ltr">reserve_lock = no</code>. If the attribute is <code dir="ltr">reserve_policy</code>, then ensure that the setting is <code dir="ltr">reserve_policy = no_reserve</code>.</p>
<p>If necessary, change the setting with the <code dir="ltr">chdev</code> command using the following syntax, where <span class="italic">n</span> is the hdisk device number:</p>
<pre dir="ltr">chdev -l hdisk<span class="italic">n</span> -a [ reserve_lock=no | reserve_policy=no_reserve ]
</pre>
<p>For example, to change a setting for the device <code dir="ltr">hdisk4</code> from <code dir="ltr">reserve_lock=yes</code> to <code dir="ltr">reserve_lock=no</code>, enter the following command:</p>
<pre dir="ltr"># chdev -l hdisk4  -a  reserve_lock=no
</pre>
<p>To verify that the setting is correct on all disk devices, enter the following command:</p>
<pre dir="ltr"># lsattr -El hdisk<span class="italic">n</span> | grep reserve
</pre></li>
<li>
<p>Enter commands similar to the following on any node to clear the PVID from each disk device that you want to use:</p>
<pre dir="ltr"># /usr/sbin/chdev -l hdisk<span class="italic">n</span> -a pv=clear
</pre>
<p>When you are installing Oracle Clusterware, you must enter the paths to the appropriate device files when prompted for the path of the OCR and Oracle Clusterware voting disk. For example:</p>
<pre dir="ltr">/dev/rhdisk10
</pre></li>
</ol>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="CDEHFDGG"></a>
<div id="CWAIX277" class="sect2">
<h3 class="sect2"><span class="secnum">6.4.2</span> Using Disk Groups with Oracle Database Files on Oracle ASM</h3>
<p>Review the following sections to configure Oracle Automatic Storage Management storage for Oracle Clusterware and Oracle Database Files:</p>
<ul>
<li>
<p><a href="#CDECEJHB">Identifying and Using Existing Oracle Database Disk Groups on Oracle ASM</a></p>
</li>
<li>
<p><a href="#CDEIFJBJ">Creating Disk Groups for Oracle Database Data Files</a></p>
</li>
</ul>
<a id="CDECEJHB"></a>
<div id="CWAIX278" class="sect3"><!-- infolevel="all" infotype="General" -->
<h4 class="sect3"><span class="secnum">6.4.2.1</span> Identifying and Using Existing Oracle Database Disk Groups on Oracle ASM</h4>
<p>The following section describes how to identify existing disk groups and determine the free disk space that they contain.</p>
<ul>
<li>
<p>Optionally, identify failure groups for the Oracle Automatic Storage Management disk group devices.<a id="sthref641"></a><a id="sthref642"></a><a id="sthref643"></a></p>
<p><a id="sthref644"></a><a id="sthref645"></a><a id="sthref646"></a><a id="sthref647"></a><a id="sthref648"></a>If you intend to use a normal or high redundancy disk group, then you can further protect your database against hardware failure by associating a set of disk devices in a custom failure group. By default, each device comprises its own failure group. However, if two disk devices in a normal redundancy disk group are attached to the same SCSI controller, then the disk group becomes unavailable if the controller fails. The controller in this example is a single point of failure.</p>
<p>To protect against failures of this type, you could use two SCSI controllers, each with two disks, and define a failure group for the disks attached to each controller. This configuration would enable the disk group to tolerate the failure of one SCSI controller.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
If you define custom failure groups, then you must specify a minimum of two failure groups for normal redundancy and three failure groups for high redundancy.</div>
</li>
</ul>
</div>
<!-- class="sect3" -->
<a id="CDEIFJBJ"></a>
<div id="CWAIX279" class="sect3">
<h4 class="sect3"><span class="secnum">6.4.2.2</span> Creating Disk Groups for Oracle Database Data Files</h4>
<p>If you are sure that a suitable disk group does not exist on the system, then install or identify appropriate disk devices to add to a new disk group. Use the following guidelines when identifying appropriate disk devices:</p>
<ul>
<li>
<p>All of the devices in an Oracle Automatic Storage Management disk group should be the same size and have the same performance characteristics.</p>
</li>
<li>
<p>Do not specify multiple partitions on a single physical disk as a disk group device. Oracle Automatic Storage Management expects each disk group device to be on a separate physical disk.</p>
</li>
<li>
<p>Although you can specify logical volumes as devices in an Oracle ASM disk group, Oracle does not recommend their use. Non-shared logical volumes are not supported with Oracle RAC. If you want to use logical volumes for your Oracle RAC database, then you must use shared and concurrent raw logical volumes created by a cluster-aware logical volume manager.</p>
</li>
</ul>
</div>
<!-- class="sect3" -->
<div class="sect3"><!-- infolevel="all" infotype="General" --><a id="sthref649"></a>
<h4 class="sect3"><span class="secnum">6.4.2.3</span> Creating and Using Oracle ASM Credentials File</h4>
<p>An Oracle ASM Storage Client does not have Oracle ASM running on the nodes and uses Oracle ASM storage services in a different client cluster.</p>
<p>To create Oracle ASM credentials file, from the <code dir="ltr"><span class="codeinlineitalic">Grid_home</span></code><code dir="ltr">/bin</code> directory on the Storage Server, run the following command on one of the member nodes, where <span class="italic">credential_file</span> is the name and path location of the Oracle ASM credentials file you create:</p>
<pre dir="ltr"><span class="italic">Grid_home</span>/bin/asmcmd mkcc <span class="codeinlineitalic">client_cluster_name credential_file</span>
</pre>
<p>For example:</p>
<pre dir="ltr"><span class="italic">Grid_home</span>/bin/asmcmd mkcc clientcluster1 /home/grid/clientcluster1_credentials.xml
</pre>
<p>Copy the Oracle ASM credentials file to a secure path on the client cluster node where you run the client cluster installation. The Oracle Installation user must have permissions to access that file. Oracle recommends that no other user is granted permissions to access the Oracle ASM credentials file. During installation, you are prompted to provide a path to the file.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
<ul>
<li>The Oracle ASM credentials file can be used only once. If an Oracle ASM Storage Client is configured and deconfigured, you must create a new Oracle ASM credentials file.</li>
<li>
<p>If the Oracle ASM credentials file is used to configure the client cluster, then it cannot be shared or reused to configure another client cluster.</p>
</li>
</ul>
</div>
</div>
<!-- class="sect3" --></div>
<!-- class="sect2" -->
<a id="BABFJECI"></a>
<div id="CWAIX436" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.4.3</span> Configuring Oracle Automatic Storage Management Cluster File System</h3>
<p>Oracle ACFS is installed as part of an Oracle Grid Infrastructure installation (Oracle Clusterware and Oracle Automatic Storage Management) for 12<span class="italic">c</span> release 1 (12.1). You can configure Oracle ACFS for a database home, or use ASMCA to configure ACFS as a general purpose file system.</p>
<div class="infobox-note">
<p class="notep1">Note:</p>
Oracle ACFS is supported only on AIX 6.1 TL4 SP2 and later updates to AIX 6.1 (on PPC64 only). Starting with Oracle Grid Infrastructure for a Cluster 11g Release 2 (11.2.0.3), Oracle ACFS is supported on all technical levels of AIX 7.1.</div>
<p>To configure Oracle ACFS for an Oracle Database home for an Oracle RAC database:</p>
<ol>
<li>
<p>Install Oracle Grid Infrastructure for a cluster (Oracle Clusterware and Oracle Automatic Storage Management)</p>
</li>
<li>
<p>Change directory to the Oracle Grid Infrastructure home. For example:</p>
<pre dir="ltr">$ cd /u01/app/12.1.0/grid
</pre></li>
<li>
<p>Ensure that the Oracle Grid Infrastructure installation owner has read and write permissions on the storage mountpoint you want to use. For example, if you want to use the mountpoint <code dir="ltr">/u02/acfsmounts/</code>:</p>
<pre dir="ltr">$ ls -l /u02/acfsmounts
</pre></li>
<li>
<p><a id="sthref650"></a>Start Oracle ASM Configuration Assistant as the grid installation owner. For example:</p>
<pre dir="ltr">./asmca
</pre></li>
<li>
<p>The Configure ASM: ASM Disk Groups page shows you the Oracle ASM disk group you created during installation. Click the <span class="bold">ASM Cluster File Systems</span> tab.</p>
</li>
<li>
<p>On the ASM Cluster File Systems page, right-click the Data disk, then select <span class="bold">Create ACFS for Database Home</span>.</p>
</li>
<li>
<p>In the Create ACFS Hosted Database Home window, enter the following information:</p>
<ul>
<li>
<p><span class="bold">Database Home ADVM Volume Device Name</span>: Enter the name of the database home. The name must be unique in your enterprise. For example: <code dir="ltr">dbase_01</code></p>
</li>
<li>
<p><span class="bold">Database Home Mountpoint</span>: Enter the directory path for the mount point. For example: <code dir="ltr">/u02/acfsmounts/dbase_01</code></p>
<p>Make a note of this mount point for future reference.</p>
</li>
<li>
<p><span class="bold">Database Home Size (GB</span>): Enter in gigabytes the size you want the database home to be.</p>
</li>
<li>
<p><span class="bold">Database Home Owner Name</span>: Enter the name of the Oracle Database installation owner you plan to use to install the database. For example: <code dir="ltr">oracle</code>1</p>
</li>
<li>
<p><span class="bold">Database Home Owner Group</span>: Enter the OSDBA group whose members you plan to provide when you install the database. Members of this group are given operating system authentication for the SYSDBA privileges on the database. For example: <code dir="ltr">dba1</code></p>
</li>
<li>
<p>Click <span class="bold">OK</span> when you have completed your entries.</p>
</li>
</ul>
</li>
<li>
<p>Run the script generated by Oracle ASM Configuration Assistant as a privileged user (<code dir="ltr">root</code>). On an Oracle Clusterware environment, the script registers the ACFS as a resource managed by Oracle Clusterware. Registering ACFS as a resource helps Oracle Clusterware to mount the ACFS automatically in proper order when ACFS is used for an Oracle RAC database Home.</p>
</li>
<li>
<p>During Oracle RAC installation, ensure that you or the DBA who installs Oracle RAC selects for the Oracle home the mount point you provided in the <span class="bold">Database Home Mountpoint</span> field (in the preceding example, <code dir="ltr">/u02/acfsmounts/dbase_01</code>).</p>
</li>
</ol>
<div class="infoboxnotealso">
<p class="notep1">See Also:</p>
<a class="olink OSTMG" href="../OSTMG/toc.htm"><span class="italic">Oracle Automatic Storage Management Administrator&#39;s Guide</span></a> for more information about configuring and managing your storage with Oracle ACFS</div>
</div>
<!-- class="sect2" -->
<a id="CDEEAIHI"></a>
<div id="CWAIX280" class="sect2"><!-- infolevel="all" infotype="General" -->
<h3 class="sect2"><span class="secnum">6.4.4</span> Upgrading Existing Oracle ASM Instances</h3>
<p>If you have an Oracle ASM installation from a prior release installed on your server, or in an existing Oracle Clusterware installation, then you can use Oracle Automatic Storage Management Configuration Assistant (ASMCA, located in the path <code dir="ltr"><span class="codeinlineitalic">Grid_home</span></code><code dir="ltr">/bin</code>) to upgrade the existing Oracle ASM instance to 12<span class="italic">c</span> Release 1 (12.1), and subsequently configure failure groups, Oracle ASM volumes and Oracle Automatic Storage Management Cluster File System (Oracle ACFS).</p>
<div class="infoboxnotealso">
<p class="notep1">Note:</p>
You must first shut down all database instances and applications on the node with the existing Oracle ASM instance before upgrading it.</div>
<p>During installation, if you are upgrading from an Oracle ASM release before 11.2, and you chose to use Oracle ASM and ASMCA detects that there is a prior Oracle ASM version installed in another Oracle ASM home, then after installing the Oracle ASM 12<span class="italic">c</span> Release 1 (12.1) binaries, you can start ASMCA to upgrade the existing Oracle ASM instance. You can then choose to configure an Oracle ACFS deployment by creating Oracle ASM volumes and using the upgraded Oracle ASM to create the Oracle ACFS.</p>
<p>If you are upgrading from Oracle ASM 11<span class="italic">g</span> Release 2 (11.2.0.1) or later, then Oracle ASM is always upgraded with Oracle Grid Infrastructure as part of the rolling upgrade, and ASMCA is started by the root scripts during upgrade. ASMCA cannot perform a separate upgrade of Oracle ASM from a prior release to the current release.</p>
<p>On an existing Oracle Clusterware or Oracle RAC installation, if the prior version of Oracle ASM instances on all nodes is 11<span class="italic">g</span> Release 1 or later, then you are provided with the option to perform a rolling upgrade of Oracle ASM instances. If the earlier version of Oracle ASM instances on an Oracle RAC installation are from a release before 11<span class="italic">g</span> Release 1, then rolling upgrades cannot be performed. In that case, Oracle ASM on all nodes are upgraded to 12<span class="italic">c</span> Release 1 (12.1).</p>
</div>
<!-- class="sect2" --></div>
<!-- class="sect1" --></div>
<!-- class="chapter" --></div>
<!-- class="ind" -->
<!-- Start Footer -->
</div>
<!-- add extra wrapper close div-->
<footer><!--
<hr />
<table class="cellalignment4190">
<tr>
<td class="cellalignment4197">
<table class="cellalignment4195">
<tr>
<td class="cellalignment4194"><a href="usrgrps.htm"><img width="24" height="24" src="../dcommon/gifs/leftnav.gif" alt="Go to previous page" /><br />
<span class="icon">Previous</span></a></td>
<td class="cellalignment4194"><a href="crsunix.htm"><img width="24" height="24" src="../dcommon/gifs/rightnav.gif" alt="Go to next page" /><br />
<span class="icon">Next</span></a></td>
</tr>
</table>
</td>
<td class="cellalignment-copyrightlogo"><img width="144" height="18" src="../dcommon/gifs/oracle.gif" alt="Oracle" /><br />
Copyright&nbsp;&copy;&nbsp;2014, 2017,&nbsp;Oracle&nbsp;and/or&nbsp;its&nbsp;affiliates.&nbsp;All&nbsp;rights&nbsp;reserved.<br />
<a href="../dcommon/html/cpyr.htm">Legal Notices</a></td>
<td class="cellalignment4199">
<table class="cellalignment4193">
<tr>
<td class="cellalignment4194"><a href="../index.htm"><img width="24" height="24" src="../dcommon/gifs/doclib.gif" alt="Go to Documentation Home" /><br />
<span class="icon">Home</span></a></td>
<td class="cellalignment4194"><a href="../nav/portal_booklist.htm"><img width="24" height="24" src="../dcommon/gifs/booklist.gif" alt="Go to Book List" /><br />
<span class="icon">Book List</span></a></td>
<td class="cellalignment4194"><a href="toc.htm"><img width="24" height="24" src="../dcommon/gifs/toc.gif" alt="Go to Table of Contents" /><br />
<span class="icon">Contents</span></a></td>
<td class="cellalignment4194"><a href="index.htm"><img width="24" height="24" src="../dcommon/gifs/index.gif" alt="Go to Index" /><br />
<span class="icon">Index</span></a></td>
<td class="cellalignment4194"><a href="../nav/mindx.htm"><img width="24" height="24" src="../dcommon/gifs/masterix.gif" alt="Go to Master Index" /><br />
<span class="icon">Master Index</span></a></td>
<td class="cellalignment4194"><a href="../dcommon/html/feedback.htm"><img width="24" height="24" src="../dcommon/gifs/feedbck2.gif" alt="Go to Feedback page" /><br />
<span class="icon">Contact Us</span></a></td>
</tr>
</table>
</td>
</tr>
</table>
--></footer>


</body></html>